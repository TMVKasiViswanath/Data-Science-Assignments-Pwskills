{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc37a8-d436-4b58-9038-2e8ddcd756fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "R-squared, also known as the coefficient of determination, is a statistical measure that represents \n",
    "the proportion of the variance in the dependent variable that is predictable from the independent\n",
    "variables in a regression model. In other words, it indicates how well the independent variables \n",
    "explain the variability of the dependent variable.\n",
    "\n",
    "R-squared ranges from 0 to 1, with 0 indicating that the model does not explain any of the variability\n",
    "of the dependent variable, and 1 indicating that the model explains all the variability. However, it\n",
    "important to note that a high R-squared does not necessarily mean that the model is good, as it can \n",
    "be artificially inflated by adding more independent variables, even if they are not relevant to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bb139-19eb-434c-b283-61d3b1aba36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a\n",
    "regression model. It is particularly useful when comparing models with different numbers of predictors,\n",
    "as it penalizes the addition of unnecessary variables that do not improve the model performance.\n",
    "\n",
    "Adjusted R-squared will always be less than or equal to R-squared. If adding a new variable to the model\n",
    "does not improve the models fit, the adjusted R-squared will decrease, reflecting the penalty for the\n",
    "additional variable. If adding a new variable does improve the model, the adjusted R-squared will increase.\n",
    "Adjusted R-squared is often preferred for model comparison because it provides a more accurate reflection\n",
    "of a model explanatory power, especially when comparing models with different numbers of predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bdca1-fc47-46d4-891c-a40646c7cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing regression models with different numbers\n",
    "of predictors. It helps in determining whether adding more predictors to the model actually improves \n",
    "the model performance or if it is just adding unnecessary complexity. Adjusted R-squared penalizes \n",
    "models with more predictors, so it is useful for selecting the most parsimonious model that still \n",
    "explains the data well.\n",
    "\n",
    "Adjusted R-squared is particularly useful in situations where there are many potential predictors\n",
    "to choose from, as it helps in selecting the most relevant ones and avoiding overfitting. It is also\n",
    "useful when interpreting the overall fit of the model, as it provides a more accurate measure of the\n",
    "model explanatory power when compared to R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230868e-9887-455c-897b-e5a663a1808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean Absolute Error (MAE):\n",
    "\n",
    "MAE is the average of the absolute differences between the predicted values and the actual values.\n",
    "It represents the average magnitude of the errors in the predictions.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "MSE is the average of the squared differences between the predicted values and the actual values.\n",
    "It gives more weight to larger errors compared to MAE\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is the square root of the MSE.\n",
    "It is in the same units as the dependent variable, which makes it easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1473a-0995-45b1-aa8e-d19b322ec89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE:\n",
    "\n",
    "Advantages:\n",
    "Easy to understand.\n",
    "Less affected by outliers.\n",
    "Disadvantages:\n",
    "Doesn emphasize large errors.\n",
    "MSE:\n",
    "\n",
    "Advantages:\n",
    "Penalizes large errors more.\n",
    "Useful for optimization.\n",
    "Disadvantages:\n",
    "Harder to interpret.\n",
    "Sensitive to outliers.\n",
    "RMSE:\n",
    "\n",
    "Advantages:\n",
    "In same units as dependent variable.\n",
    "Penalizes large errors.\n",
    "Disadvantages:\n",
    "Sensitive to outliers.\n",
    "Can be influenced by large errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffd3aa-af13-4ea5-9433-dafe61e05906",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regularization:\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a technique in regression that helps prevent\n",
    "overfitting by adding a penalty for having large coefficients. It works by shrinking some coefficients\n",
    "to zero, effectively removing those features from the model.\n",
    "\n",
    "Differences from Ridge Regularization:\n",
    "Ridge regularization also prevents overfitting by adding a penalty term, but it uses the squared values\n",
    "of coefficients. This tends to shrink coefficients towards zero without making them exactly zero.\n",
    "\n",
    "When to Use Lasso:\n",
    "Lasso is preferred when:\n",
    "\n",
    "You have many features, and some are likely irrelevant.\n",
    "You want a simpler, more interpretable model with fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24013e-d25b-4dd0-8cb1-19e4a4b24b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a\n",
    "penalty term to the standard linear regression objective function. This penalty term discourages the\n",
    "model from learning complex patterns that might fit the training data very closely but generalize poorly\n",
    "to new, unseen data. There are two common types of regularization used in linear models: Lasso \n",
    "(L1 regularization) and Ridge (L2 regularization).\n",
    "\n",
    "Lasso (L1 regularization):\n",
    "In Lasso regularization, the penalty term is the sum of the absolute values of the coefficients \n",
    "multiplied by a regularization parameter (alpha). This penalty encourages sparsity in the model,\n",
    "meaning it tends to force some of the coefficients to be exactly zero, effectively performing feature \n",
    "selection by eliminating less important variables from the model.\n",
    "\n",
    "Ridge (L2 regularization):\n",
    "In Ridge regularization, the penalty term is the sum of the squared coefficients multiplied by a\n",
    "regularization parameter (alpha). This penalty shrinks the coefficients towards zero but does not\n",
    "usually result in coefficients being exactly zero. It helps to reduce the impact of irrelevant or \n",
    "redundant features on the model.\n",
    "\n",
    "Example:\n",
    "Let say we have a dataset with 100 features, but only 10 of them are truly important for predicting \n",
    "the target variable. Without regularization, the linear model might try to fit all 100 features, \n",
    "leading to potential overfitting. By using Lasso regularization, we can encourage the model to focus \n",
    "on the most important features and set the coefficients of the less important features to zero. This \n",
    "helps prevent overfitting and improves the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a15c6-b73e-4d4c-82dc-e40dcd8be563",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Selection Bias: Regularized linear models like Lasso tend to select a subset of features by\n",
    "setting some coefficients to zero. This can lead to bias in feature selection, especially when there are\n",
    "correlated features.\n",
    "\n",
    "Over-regularization: If the regularization parameter is too large, the model may be overly simplified, \n",
    "leading to underfitting and poor performance on both training and test data.\n",
    "\n",
    "Sensitive to Scaling: Regularized linear models are sensitive to the scale of the features. Features \n",
    "with larger scales may dominate the regularization penalty, leading to biased coefficients.\n",
    "\n",
    "Model Interpretability: While regularization helps prevent overfitting, it can also make the model less\n",
    "interpretable, especially when many coefficients are set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f6100-e65d-4f69-86c4-1586528636dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model A (RMSE of 10): Model A has a slightly higher average error but is more sensitive to large errors \n",
    "or outliers.\n",
    "\n",
    "Model B (MAE of 8): Model B has a slightly lower average error but treats all errors equally, without\n",
    "giving extra weight to large errors.\n",
    "\n",
    "Choosing Between the Models:\n",
    "\n",
    "If you want a model that is more robust to outliers and large errors, Model A might be better, as it\n",
    "has a higher RMSE.\n",
    "\n",
    "If you want a model with lower average error, regardless of the size of the errors, Model B might be\n",
    "better, as it has a lower MAE.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Both metrics have limitations. RMSE can be influenced by outliers, while MAE does not provide information\n",
    "about the variance of the errors.\n",
    "\n",
    "The choice of metric should consider the specific requirements of the problem and the importance of \n",
    "different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b046b-bdac-4c97-b51f-6036ff4c9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "In comparing two regularized linear models, Ridge (Model A) and Lasso (Model B), with different \n",
    "regularization parameters (0.1 for Ridge and 0.5 for Lasso), the choice depends on the importance \n",
    "of feature selection and model interpretability:\n",
    "\n",
    "Ridge (Model A):\n",
    "\n",
    "Retains all features but reduces the impact of less important ones.\n",
    "Regularization parameter of 0.1 indicates moderate penalty for large coefficients.\n",
    "Lasso (Model B):\n",
    "\n",
    "Can perform feature selection by setting some coefficients to zero.\n",
    "Regularization parameter of 0.5 indicates stronger penalty, potentially leading to more coefficients\n",
    "being set to zero.\n",
    "Choosing the Better Model:\n",
    "\n",
    "Model A (Ridge):\n",
    "\n",
    "Good for many features with small to medium effects.\n",
    "May be more stable but less interpretable.\n",
    "Model B (Lasso):\n",
    "\n",
    "Preferred for many features with some likely irrelevant ones.\n",
    "More interpretable but can be biased in feature selection.\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Feature Selection: Lasso can select features but may be biased with correlated features.\n",
    "Interpretability: Lasso is more interpretable due to zeroed-out coefficients.\n",
    "Regularization Parameter: Needs careful tuning to balance overfitting and underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
