{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec35df00-3d1f-4e37-a225-2f858eaf08c9",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "An activation function in artificial neural networks is a mathematical function applied to the output of each neuron (or unit) in a neural network. It determines whether a neuron should be activated (i.e., fire) based on the weighted sum of its inputs. Activation functions introduce non-linearity into the network, allowing it to learn complex patterns in the data.\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "Sigmoid: Outputs values between 0 and 1, often used in the output layer for binary classification tasks.\n",
    "\n",
    "Tanh: Outputs values between -1 and 1, similar to sigmoid but centered at zero.\n",
    "\n",
    "ReLU (Rectified Linear Unit): Outputs the input directly if positive, otherwise outputs zero. It's widely used due to its simplicity and effectiveness in training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eec0b8-d867-49cb-8389-90b9ed23b0a9",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Sigmoid Function (Logistic):\n",
    "\n",
    "ùúé\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "ùëí\n",
    "‚àí\n",
    "ùë•\n",
    "œÉ(x)= \n",
    "1+e \n",
    "‚àíx\n",
    " \n",
    "1\n",
    "‚Äã\n",
    " \n",
    "Outputs values between 0 and 1.\n",
    "Useful for binary classification tasks in the output layer.\n",
    "Hyperbolic Tangent Function (Tanh):\n",
    "\n",
    "tanh\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "ùëí\n",
    "ùë•\n",
    "‚àí\n",
    "ùëí\n",
    "‚àí\n",
    "ùë•\n",
    "ùëí\n",
    "ùë•\n",
    "+\n",
    "ùëí\n",
    "‚àí\n",
    "ùë•\n",
    "tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "‚àíx\n",
    " \n",
    "e \n",
    "x\n",
    " ‚àíe \n",
    "‚àíx\n",
    " \n",
    "‚Äã\n",
    " \n",
    "Outputs values between -1 and 1.\n",
    "Similar to sigmoid but zero-centered, often used in hidden layers.\n",
    "Rectified Linear Unit (ReLU):\n",
    "\n",
    "ReLU\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "max\n",
    "‚Å°\n",
    "(\n",
    "0\n",
    ",\n",
    "ùë•\n",
    ")\n",
    "ReLU(x)=max(0,x)\n",
    "Outputs the input if it's positive, otherwise outputs zero.\n",
    "Simple and effective, commonly used in hidden layers.\n",
    "Leaky ReLU:\n",
    "\n",
    "Leaky¬†ReLU\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "{\n",
    "ùë•\n",
    ",\n",
    "if¬†\n",
    "ùë•\n",
    ">\n",
    "0\n",
    "ùõº\n",
    "ùë•\n",
    ",\n",
    "otherwise\n",
    "Leaky¬†ReLU(x)={ \n",
    "x,\n",
    "Œ±x,\n",
    "‚Äã\n",
    "  \n",
    "if¬†x>0\n",
    "otherwise\n",
    "‚Äã\n",
    " \n",
    "where \n",
    "ùõº\n",
    "Œ± is a small constant (e.g., 0.01).\n",
    "\n",
    "Addresses the \"dying ReLU\" problem where neurons could become inactive during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe52c86-87d7-4648-8c9a-f6012412ba59",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Non-linearity: Activation functions introduce non-linearity into the network, allowing it to learn and approximate complex mappings between inputs and outputs. Without non-linear activation functions, a neural network would behave like a linear model, unable to capture intricate patterns in data.\n",
    "\n",
    "Gradient Propagation: During backpropagation, which is used to update weights in a neural network, the derivative of the activation function determines how error gradients are propagated through the network. Smooth activation functions with well-defined derivatives (like sigmoid, tanh, and ReLU variants) facilitate stable and efficient gradient propagation, enabling effective learning.\n",
    "\n",
    "Avoiding Vanishing and Exploding Gradients: Certain activation functions (e.g., sigmoid and tanh) can suffer from vanishing gradient problems, where gradients become very small as activations move away from the origin, leading to slower learning or stagnant training. On the other hand, activation functions like ReLU and its variants mitigate this issue by allowing for more effective gradient flow, which helps in faster convergence during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2dd9f9-8f0c-4434-980c-25e77189ebfe",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Working of Sigmoid Activation Function:\n",
    "Range: The sigmoid function outputs values between 0 and 1, which can be interpreted as probabilities. Specifically, \n",
    "ùúé\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "œÉ(x) tends towards 1 as \n",
    "ùë•\n",
    "x tends to infinity, and towards 0 as \n",
    "ùë•\n",
    "x tends to negative infinity.\n",
    "\n",
    "Smoothness: It is a smooth function that is continuously differentiable, which allows for gradient-based optimization methods like gradient descent to be applied during training.\n",
    "\n",
    "Output Interpretation: In neural networks, the sigmoid function is often used in the output layer for binary classification tasks. The output can be interpreted as the probability of a sample belonging to a particular class.\n",
    "\n",
    "Advantages of Sigmoid Activation Function:\n",
    "Output Interpretation: Outputs can be interpreted as probabilities, which is useful for binary classification tasks where the network needs to predict probabilities of class membership.\n",
    "\n",
    "Smooth Gradient: The derivative of the sigmoid function is straightforward and is expressed in terms of the function itself, making gradient calculations easy during backpropagation.\n",
    "\n",
    "Disadvantages of Sigmoid Activation Function:\n",
    "Vanishing Gradient: Sigmoid activations saturate and flatten when inputs are very large or very small, leading to vanishing gradients. This can slow down the learning process, especially in deep networks.\n",
    "\n",
    "Not Zero-Centered: The outputs of the sigmoid function are not zero-centered (they range from 0 to 1), which can make optimization trickier compared to zero-centered functions like ReLU.\n",
    "\n",
    "Output Saturation: Sigmoid activations can lead to saturation of neurons, where neurons stop learning completely because they are stuck in the saturated regime (output close to 0 or 1), especially during the backpropagation of gradients.\n",
    "\n",
    "Not Suitable for Hidden Layers: Due to the vanishing gradient problem and issues with saturation, sigmoid activations are less commonly used in hidden layers of deep neural networks compared to ReLU and its variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db29ea07-8564-44eb-ae06-d818b6e07bfd",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "The Rectified Linear Unit (ReLU) activation function is a non-linear function widely used in deep learning and neural networks. It is defined as:\n",
    "\n",
    "ReLU\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "max\n",
    "‚Å°\n",
    "(\n",
    "0\n",
    ",\n",
    "ùë•\n",
    ")\n",
    "ReLU(x)=max(0,x)\n",
    "\n",
    "Differences from Sigmoid Function:\n",
    "\n",
    "Output Range: Sigmoid function outputs values between 0 and 1, while ReLU outputs values between 0 and infinity for positive inputs.\n",
    "\n",
    "Linearity vs. Non-Linearity: Sigmoid function is non-linear throughout its range, whereas ReLU is piecewise linear (linear for \n",
    "ùë•\n",
    ">\n",
    "0\n",
    "x>0 and zero for \n",
    "ùë•\n",
    "‚â§\n",
    "0\n",
    "x‚â§0).\n",
    "\n",
    "Gradient Behavior: Sigmoid function has a smooth derivative and does not abruptly cut off gradients, but it suffers from vanishing gradients for extreme inputs. ReLU, on the other hand, has a derivative of 1 for \n",
    "ùë•\n",
    ">\n",
    "0\n",
    "x>0 and zero for \n",
    "ùë•\n",
    "‚â§\n",
    "0\n",
    "x‚â§0, which can lead to more efficient gradient propagation in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1cc7d-ffff-426c-a915-3c19e54e391d",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "Avoids Vanishing Gradient Problem:\n",
    "\n",
    "Sigmoid activations can lead to vanishing gradients, especially for large or small input values, which slows down or prevents effective learning in deep networks. ReLU, on the other hand, does not suffer from this issue for positive inputs, as its gradient remains constant (1) for \n",
    "ùë•\n",
    ">\n",
    "0\n",
    "x>0.\n",
    "Faster Convergence:\n",
    "\n",
    "ReLU typically leads to faster convergence during training compared to sigmoid. This is because ReLU's linear nature for positive inputs allows gradients to flow more freely through the network, facilitating quicker updates to network weights.\n",
    "Sparse Activation:\n",
    "\n",
    "ReLU promotes sparsity by zeroing out negative inputs. This sparsity can lead to more efficient computation and memory usage in neural networks, as fewer neurons are activated at any given time.\n",
    "Computational Efficiency:\n",
    "\n",
    "ReLU is computationally more efficient than sigmoid and other activation functions that involve complex mathematical operations (e.g., exponentials). ReLU involves simple operations like comparison and max function, making it faster to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e223c-67ff-494c-a499-ff96e3446dda",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "\"Leaky ReLU\" is a variant of the Rectified Linear Unit (ReLU) activation function, which is commonly used in neural networks. Unlike the standard ReLU function, which sets all negative values to zero, Leaky ReLU allows a small, positive gradient (alpha * x where alpha is a small constant, typically 0.01) for negative inputs.\n",
    "\n",
    "Here's how Leaky ReLU addresses the vanishing gradient problem:\n",
    "\n",
    "Vanishing Gradient Problem: In deep neural networks during backpropagation, gradients can become very small (approaching zero) as they propagate backward through layers. This can hinder the training process, as small gradients lead to slow learning or no learning at all.\n",
    "\n",
    "Leaky ReLU Solution: By allowing a small, non-zero gradient for negative inputs (unlike ReLU which sets them to zero), Leaky ReLU helps mitigate the vanishing gradient problem. This ensures that neurons in the network continue to receive updates even for negative inputs, which can help maintain and propagate gradients backward through the network during training.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Prevents Dead Neurons: Neurons in ReLU can sometimes become \"dead\" if they consistently output zero (especially during large learning rates or due to negative bias). Leaky ReLU prevents this by allowing some gradient flow.\n",
    "Stable Learning: It provides a more stable gradient and prevents the problem of gradients approaching zero, thus aiding in faster and more consistent learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3386043-0bc6-4b9e-ad87-2f10624401f5",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "The softmax activation function is primarily used in multi-class classification problems where the goal is to output probabilities that sum up to 1 across all classes. Here's a breakdown of its purpose and common usage:\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Probability Distribution: Softmax converts logits (raw predictions) into probabilities that represent the likelihood of each class being the correct one.\n",
    "Output Interpretation: It ensures that the output values are non-negative and sum up to 1, making it suitable for interpreting the outputs as probabilities.\n",
    "Usage:\n",
    "\n",
    "Multi-class Classification: Softmax is commonly used as the final activation function in neural networks for multi-class classification tasks.\n",
    "Output Layer: It is typically applied on the output layer of a neural network when the network is trained to predict multiple mutually exclusive classes.\n",
    "Loss Calculation: Softmax is often paired with the categorical cross-entropy loss function to compute the loss based on the predicted probabilities compared to the true class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c53be-b51b-4ba1-8843-93ef31806bff",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "The hyperbolic tangent (tanh) activation function is another popular choice in neural networks, similar to the sigmoid function but with some distinct characteristics. Here's an overview of tanh and its comparison to the sigmoid function:\n",
    "\n",
    "Definition:\n",
    "\n",
    "The tanh function is defined as:\n",
    "\n",
    "tanh(x)= \n",
    "e \n",
    "x\n",
    " +e \n",
    "‚àíx\n",
    " /\n",
    "e \n",
    "x\n",
    " ‚àíe \n",
    "‚àíx\n",
    " \n",
    "‚Äã\n",
    " \n",
    "It outputs values between -1 and 1, mapping any real-valued input to this range.\n",
    "Properties:\n",
    "\n",
    "Range: Outputs of tanh are in the interval [-1, 1], which means it is zero-centered (unlike sigmoid, which is centered around 0.5).\n",
    "Symmetry: The function is symmetric around the origin, i.e., \n",
    "tanh\n",
    "(\n",
    "‚àí\n",
    "ùë•\n",
    ")\n",
    "=\n",
    "‚àí\n",
    "tanh\n",
    "(\n",
    "ùë•\n",
    ")\n",
    "tanh(‚àíx)=‚àítanh(x).\n",
    "Smoothness: tanh is smooth and differentiable everywhere, similar to sigmoid.\n",
    "Comparison with Sigmoid:\n",
    "\n",
    "Range: Sigmoid outputs values between 0 and 1, making it suitable for binary classification tasks where the output represents a probability.\n",
    "Zero-Centered: tanh is zero-centered, which can aid in faster convergence of the gradient descent algorithm during training, especially for deep neural networks.\n",
    "Output Scaling: Outputs from tanh are scaled between -1 and 1, potentially leading to stronger gradients compared to sigmoid, which can help mitigate the vanishing gradient problem to some extent.\n",
    "Practical Usage: tanh is often used in hidden layers of neural networks for tasks like image recognition and natural language processing, where inputs and outputs are not binary but range over a broader spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62cc48-32be-4335-9722-d2f4c8474d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
