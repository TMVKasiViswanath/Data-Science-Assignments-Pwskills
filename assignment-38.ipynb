{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e34595-c2ea-4e52-ad63-5e942b7ecdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression is used for predicting continuous values, while logistic \n",
    "regression is used for predicting the probability of a binary outcome.\n",
    "\n",
    "For example, linear regression could be used to predict the price of a house based on its features like\n",
    "size, number of bedrooms, etc. This is a continuous value prediction task.\n",
    "\n",
    "Logistic regression, on the other hand, would be more appropriate for predicting whether a student will\n",
    "pass or fail an exam based on study hours. Here, the outcome is binary (pass or fail), and logistic \n",
    "regression can predict the probability of passing based on the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de389cd-6604-4941-bc79-91e9e5c755c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In logistic regression, the cost function used is the binary cross-entropy loss (also known as log loss)\n",
    "The goal is to minimize this cost function by adjusting the model parameters \n",
    "θ. This is typically done using an optimization algorithm such as gradient descent or one of its variants\n",
    "(e.g., stochastic gradient descent, mini-batch gradient descent). These algorithms iteratively update the \n",
    "parameters θ in the direction that reduces the cost function, eventually converging to a set of parameters \n",
    "that minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e08612-61a6-4a15-99e6-4c270fa1a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term \n",
    "to the cost function. The penalty term discourages the model from learning complex relationships that may\n",
    "not generalize well to unseen data.\n",
    "\n",
    "There are two common types of regularization used in logistic regression:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the sum of the absolute values of the coefficients to the cost function.\n",
    "It tends to produce sparse models by forcing some coefficients to be exactly zero, effectively selecting \n",
    "features.\n",
    "\n",
    "L2 Regularization (Ridge): Adds the sum of the squared values of the coefficients to the cost function. \n",
    "It tends to shrink the coefficients towards zero, but rarely fully eliminates them, allowing all features\n",
    "to be retained but with smaller weights.\n",
    "\n",
    "The regularization parameter λ controls the strength of regularization. A higher λ value results in stronger\n",
    "regularization, which can help prevent overfitting but may also lead to underfitting if set too high.\n",
    "\n",
    "By adding a regularization term to the cost function, the model is penalized for using large coefficients,\n",
    "discouraging it from fitting the training data too closely and making it more likely to generalize well to\n",
    "unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70a3d2-bfb1-4d49-9629-d40faef3ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a\n",
    "binary classification model, such as logistic regression, at various classification thresholds. It plots\n",
    "the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings.\n",
    "\n",
    "The ROC curve shows the trade-off between TPR and FPR. A higher area under the ROC curve (AUC) indicates\n",
    "better performance of the model. An AUC of 0.5 suggests random guessing, while an AUC of 1.0 indicates a\n",
    "perfect model.\n",
    "\n",
    "The ROC curve is used to visually assess the performance of a logistic regression model and compare\n",
    "it with other models. It helps to choose the optimal threshold for making predictions based on the model\n",
    "requirements, such as minimizing false positives or maximizing true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1765fd-6615-476c-b1d4-25417d88ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Univariate Feature Selection: This method selects features based on univariate statistical tests such as\n",
    "chi-squared test, ANOVA, or correlation with the target variable. It ranks features according to their\n",
    "scores and selects the top k features.\n",
    "\n",
    "Recursive Feature Elimination (RFE): RFE recursively removes features and fits the model until the desired\n",
    "number of features is reached. It uses the model feature weights or coefficients to determine which \n",
    "features are most important.\n",
    "\n",
    "L1 Regularization (Lasso): L1 regularization adds a penalty term to the cost function based on the \n",
    "absolute values of the coefficients. This leads to some coefficients being exactly zero, effectively\n",
    "performing feature selection by eliminating irrelevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ed5ed-55f8-4557-9fde-51f31b5640ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resampling Techniques:\n",
    "\n",
    "Oversampling: Increase the number of instances in the minority class by randomly duplicating them.\n",
    "Undersampling: Decrease the number of instances in the majority class by randomly removing instances.\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): Generate synthetic samples for the minority class\n",
    "based on the existing instances.\n",
    "Cost-sensitive Learning: Assign different costs to misclassification errors of different classes.\n",
    "Penalize misclassification of the minority class more heavily.\n",
    "\n",
    "Ensemble Methods:\n",
    "\n",
    "Bagging: Use techniques like Bootstrap aggregating (bagging) with resampling to balance the dataset.\n",
    "Boosting: Algorithms like AdaBoost, which iteratively train the model on different subsets of the data, \n",
    "can focus more on misclassified instances.\n",
    "Modify Decision Threshold: Adjust the decision threshold of the classifier to balance precision and recall,\n",
    "depending on the application requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875f5f0-11d1-46cb-b399-b58e08ced150",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity: If variables are highly correlated, remove or combine them to reduce redundancy.\n",
    "\n",
    "Overfitting: Prevent the model from fitting the training data too closely by using regularization or \n",
    "cross-validation.\n",
    "\n",
    "Underfitting: Ensure the model is complex enough to capture relationships in the data by selecting\n",
    "relevant features and increasing model complexity.\n",
    "\n",
    "Class Imbalance: Balance the dataset by resampling or adjusting misclassification costs.\n",
    "\n",
    "Choosing the Right Threshold: Adjust the classification threshold based on the problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ade1e6-8af8-478c-a41f-438f528f1117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
