{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07435db5-83fb-4730-ba81-e8282d7b8fa6",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "In the context of PCA (Principal Component Analysis), a projection refers to the process of transforming data from its original high-dimensional space into a lower-dimensional space. This transformation is achieved by projecting the data onto a new set of orthogonal axes called principal components.\n",
    "\n",
    "Here's how projection is used in PCA:\n",
    "\n",
    "Calculate Principal Components: PCA identifies a new set of orthogonal axes (principal components) that best explain the variance in the data. These components are ordered by the amount of variance they explain, with the first principal component capturing the most variance, the second capturing the second most, and so on.\n",
    "\n",
    "Transform Data: Once the principal components are determined, PCA projects the original data onto these components. Each data point in the original high-dimensional space is transformed into a point in the lower-dimensional space defined by the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896a0a8-201e-4ad9-99e0-2064dde180d5",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Covariance Matrix: PCA begins by computing the covariance matrix of the original data, which represents the relationships between pairs of features. \n",
    "\n",
    "Eigenvalue Decomposition: PCA then finds the eigenvectors (principal components) and corresponding eigenvalues of the covariance matrix \n",
    "𝐶\n",
    "C. The eigenvectors are directions in the original feature space, and the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "Dimensionality Reduction: The principal components are sorted based on their corresponding eigenvalues (which indicate the amount of variance they capture). By selecting the top \n",
    "𝑘\n",
    "k eigenvectors (where \n",
    "𝑘\n",
    "k is the desired number of dimensions), PCA effectively reduces the dimensionality of the data.\n",
    "\n",
    "Objective:\n",
    "\n",
    "PCA aims to maximize the variance explained by the selected principal components. This is equivalent to minimizing the reconstruction error when the data is projected onto a lower-dimensional subspace defined by these components.\n",
    "\n",
    "The optimization problem in PCA can be stated as:\n",
    "\n",
    "max\n",
    "⁡\n",
    "𝑤\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "𝑤\n",
    "𝑘\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑘\n",
    "𝑤\n",
    "𝑖\n",
    "⊤\n",
    "𝐶\n",
    "𝑤\n",
    "𝑖\n",
    "w \n",
    "1\n",
    "​\n",
    " ,…,w \n",
    "k\n",
    "​\n",
    " \n",
    "max\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "k\n",
    "​\n",
    " w \n",
    "i\n",
    "⊤\n",
    "​\n",
    " Cw \n",
    "i\n",
    "​\n",
    " \n",
    "subject to \n",
    "𝑤\n",
    "𝑖\n",
    "⊤\n",
    "𝑤\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "w \n",
    "i\n",
    "⊤\n",
    "​\n",
    " w \n",
    "i\n",
    "​\n",
    " =1 (orthonormality constraint), where \n",
    "𝑤\n",
    "𝑖\n",
    "w \n",
    "i\n",
    "​\n",
    "  are the eigenvectors (principal components).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf71b9-4cd0-4e78-b90e-e3a79075725b",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "The covariance matrix encodes the variances of individual features along the diagonal and the covariances between different features off-diagonal. It is crucial for understanding the relationships and the structure of the data.\n",
    "Specifically, the eigenvalues of the covariance matrix indicate the amount of variance captured by each principal component, and the eigenvectors indicate the directions of these components.\n",
    "Eigenvalue Decomposition:\n",
    "\n",
    "To perform PCA, you decompose the covariance matrix \n",
    "𝐶\n",
    "C into its eigenvalues and eigenvectors. This decomposition is given by:\n",
    "𝐶\n",
    "𝑣\n",
    "=\n",
    "𝜆\n",
    "𝑣\n",
    "Cv=λv\n",
    "\n",
    "where \n",
    "𝑣\n",
    "v is an eigenvector and \n",
    "𝜆\n",
    "λ is the corresponding eigenvalue.\n",
    "The eigenvectors represent the directions (principal components) in the feature space, and the eigenvalues represent the magnitude of the variance along those directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8cf61-d790-4c91-aa80-2015193d102d",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Variance Retained:\n",
    "\n",
    "The number of principal components chosen determines how much variance from the original data is retained in the reduced-dimensional space. Generally, retaining more principal components preserves more of the original data's variance.\n",
    "PCA computes the explained variance ratio for each principal component, which indicates the proportion of variance explained by each PC. Choosing more PCs increases the cumulative explained variance, thus retaining more information.\n",
    "Dimensionality Reduction:\n",
    "\n",
    "PCA aims to reduce the dimensionality of data while minimizing information loss. The number of principal components directly determines the dimensionality of the reduced feature space.\n",
    "Choosing fewer principal components results in a more compact representation but may lead to information loss if important variance is discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d76da-175f-46c4-81a1-8b4544e712ff",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "Feature Importance:\n",
    "\n",
    "Features that contribute significantly to the variance across principal components are considered important. PCA ranks features based on their influence in forming these components.\n",
    "The first few principal components often explain a large portion of the total variance, implying that the corresponding original features are crucial for representing the data.\n",
    "Feature Projection:\n",
    "\n",
    "After PCA transforms the data into the principal component space, you can project the original features onto the principal components to determine which features are most influential in differentiating data points.\n",
    "Benefits of Using PCA for Feature Selection:\n",
    "Reduction of Redundancy:\n",
    "\n",
    "PCA identifies and removes redundancy among features by grouping them into principal components. This helps in simplifying the dataset while retaining the most informative features.\n",
    "Noise Reduction:\n",
    "\n",
    "PCA can mitigate the impact of noisy features by focusing on components that capture the largest variances, which are more likely to represent true signal rather than noise.\n",
    "Improved Model Performance:\n",
    "\n",
    "By selecting the most relevant principal components, PCA can improve the performance of machine learning models. Models trained on reduced-dimensional data often generalize better and are less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a75c1-8077-4138-8a7b-90146dafbcf4",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "PCA is primarily used to reduce the number of features (dimensions) in a dataset while retaining as much variance as possible. This reduction simplifies the data and speeds up subsequent computational tasks.\n",
    "Applications: Preprocessing high-dimensional data in areas such as image processing, text mining, and bioinformatics.\n",
    "Feature Extraction and Selection:\n",
    "\n",
    "PCA helps in identifying the most significant features (principal components) that explain the variance in the data. These components can then be used as inputs for machine learning algorithms.\n",
    "Applications: Selecting important features for classification, regression, and clustering tasks, especially in datasets with many correlated variables.\n",
    "Data Visualization:\n",
    "\n",
    "PCA transforms data into a lower-dimensional space that can be easily visualized. This aids in exploring and understanding data patterns and relationships.\n",
    "Applications: Visualizing high-dimensional datasets in fields like finance (e.g., stock market analysis), biology (e.g., gene expression analysis), and social sciences (e.g., survey data analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395175d5-f745-4f8f-ad6a-cdc96f0cef55",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "Variance in PCA: Each principal component in PCA is associated with an eigenvalue, which represents the amount of variance explained by that component. The larger the eigenvalue, the more variance is captured by the corresponding principal component.\n",
    "\n",
    "Spread in PCA: When data points are projected onto principal components, the spread refers to how the data points are distributed along each component. A larger spread along a principal component indicates that the data points vary more widely in that direction, capturing more variability from the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60a469-08e4-4d42-a74f-2999a361e9b2",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "After identifying the principal components, PCA reduces the dimensionality of the data by projecting it onto a subspace defined by these components.\n",
    "The number of principal components chosen determines the dimensionality of the reduced feature space. PCA aims to retain as much variance as possible while reducing the number of dimensions.\n",
    "Utilization of Spread and Variance:\n",
    "Spread along Principal Components: PCA identifies principal components such that the spread (variation) of data points along these components is maximized. This ensures that each principal component captures as much variability (variance) as possible.\n",
    "\n",
    "Maximization of Variance: By focusing on eigenvalues (which correspond to variance), PCA ensures that the principal components capture the maximum amount of variance present in the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b7028-eadc-4408-b100-72b134c62d1c",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "Variance Measurement:\n",
    "\n",
    "PCA begins by computing the covariance matrix of the data, which captures the variances of individual features along the diagonal and the covariances between pairs of features off-diagonal.\n",
    "Features with higher variances contribute more to the total variance of the dataset, while those with lower variances contribute less.\n",
    "Principal Component Selection:\n",
    "\n",
    "PCA identifies principal components (eigenvectors) that align with directions of maximum variance in the dataset.\n",
    "Eigenvectors associated with larger eigenvalues capture more variance and are prioritized in determining the principal components.\n",
    "Dimensionality Reduction:\n",
    "\n",
    "In the context of high variance in some dimensions and low variance in others, PCA naturally selects principal components that predominantly capture the high-variance dimensions.\n",
    "Principal components are orthogonal to each other, ensuring that each component captures independent directions of variability in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061d129-109d-4a5c-8258-c0be452bfc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
