{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363fea66-a4a5-4a8e-ad48-590879c2e9fb",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm used for classification and regression. Here's a brief overview:\n",
    "\n",
    "Classification: KNN classifies a data point based on how its neighbors are classified. It calculates the distance between the data point and its \n",
    "ùëò\n",
    "k nearest neighbors, then assigns the class most common among those neighbors.\n",
    "Regression: KNN predicts the value of a data point by averaging the values of its \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "Steps in KNN:\n",
    "\n",
    "Choose the number of neighbors \n",
    "ùëò\n",
    "k.\n",
    "Calculate the distance between the data point and all other points.\n",
    "Select the \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "For classification, assign the most common class among the neighbors. For regression, compute the average of the neighbors' values.\n",
    "Distance Metrics: Commonly used distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "\n",
    "KNN is simple and effective for small datasets but can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81867a-5c69-499c-8a2b-e14188681ab9",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Choosing the value of \n",
    "ùëò\n",
    "k in K-Nearest Neighbors (KNN) is crucial for the algorithm's performance. Here are some common methods and considerations for selecting \n",
    "ùëò\n",
    "k:\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Method: Split the dataset into multiple folds and train the KNN model with different values of \n",
    "ùëò\n",
    "k. Choose the \n",
    "ùëò\n",
    "k that results in the best cross-validation performance (e.g., accuracy for classification, mean squared error for regression).\n",
    "Reason: This helps to generalize the model performance on unseen data.\n",
    "Rule of Thumb:\n",
    "\n",
    "Method: A common heuristic is to set \n",
    "ùëò\n",
    "k to the square root of the number of data points in the training set.\n",
    "Reason: This provides a balanced starting point that can be fine-tuned with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436f8bd-27f6-4f43-96f4-78d9fb2f827b",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "KNN Classifier\n",
    "Purpose: Used for classification tasks where the output is a categorical label.\n",
    "Output: Predicts the class label based on the majority vote of the \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "Process:\n",
    "Calculate the distance between the query point and all other points in the dataset.\n",
    "Select the \n",
    "ùëò\n",
    "k nearest neighbors based on the calculated distances.\n",
    "Determine the most common class (majority vote) among these neighbors.\n",
    "Assign the query point to this class.\n",
    "Metric: Common evaluation metrics include accuracy, precision, recall, and F1-score.\n",
    "Example Use Case: Classifying emails as spam or not spam.\n",
    "KNN Regressor\n",
    "Purpose: Used for regression tasks where the output is a continuous value.\n",
    "Output: Predicts the value based on the average (or weighted average) of the \n",
    "ùëò\n",
    "k nearest neighbors.\n",
    "Process:\n",
    "Calculate the distance between the query point and all other points in the dataset.\n",
    "Select the \n",
    "ùëò\n",
    "k nearest neighbors based on the calculated distances.\n",
    "Compute the average (or weighted average) of the target values of these neighbors.\n",
    "Assign this average value as the prediction for the query point.\n",
    "Metric: Common evaluation metrics include mean squared error (MSE), mean absolute error (MAE), and \n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    "  score.\n",
    "Example Use Case: Predicting the price of a house based on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5c116-1e29-439d-bda9-5db42e075b35",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Definition: The proportion of correctly classified instances out of the total instances.\n",
    "Formula: \n",
    "Accuracy\n",
    "=\n",
    "Number¬†of¬†Correct¬†Predictions\n",
    "Total¬†Number¬†of¬†Predictions\n",
    "Accuracy= \n",
    "Total¬†Number¬†of¬†Predictions\n",
    "Number¬†of¬†Correct¬†Predictions\n",
    "‚Äã\n",
    " \n",
    "Precision:\n",
    "\n",
    "Definition: The proportion of true positive predictions out of the total predicted positives.\n",
    "Formula: \n",
    "Precision\n",
    "=\n",
    "True¬†Positives\n",
    "True¬†Positives\n",
    "+\n",
    "False¬†Positives\n",
    "Precision= \n",
    "True¬†Positives+False¬†Positives\n",
    "True¬†Positives\n",
    "‚Äã\n",
    " \n",
    "Recall (Sensitivity):\n",
    "\n",
    "Definition: The proportion of true positive predictions out of the total actual positives.\n",
    "Formula: \n",
    "Recall\n",
    "=\n",
    "True¬†Positives\n",
    "True¬†Positives\n",
    "+\n",
    "False¬†Negatives\n",
    "Recall= \n",
    "True¬†Positives+False¬†Negatives\n",
    "True¬†Positives\n",
    "‚Äã\n",
    " \n",
    "F1-Score:\n",
    "\n",
    "Definition: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "Formula: \n",
    "F1-Score\n",
    "=\n",
    "2\n",
    "√ó\n",
    "Precision\n",
    "√ó\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1-Score=2√ó \n",
    "Precision+Recall\n",
    "Precision√óRecall\n",
    "‚Äã\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b50117-a122-4101-8c09-5164453762af",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "Distance Measure Becomes Less Informative:\n",
    "\n",
    "Problem: In high-dimensional spaces, the distances between points become less distinguishable. Most points tend to be approximately equidistant from each other.\n",
    "Impact: KNN relies on distance metrics (like Euclidean distance) to identify nearest neighbors. When distances are similar, it becomes difficult to identify truly nearest neighbors, leading to poor classification or regression performance.\n",
    "Increased Computational Complexity:\n",
    "\n",
    "Problem: The time and computational resources required to calculate distances between points grow significantly with the number of dimensions.\n",
    "Impact: High-dimensional data can lead to slower training and prediction times, making KNN computationally expensive and less practical for large datasets.\n",
    "Overfitting:\n",
    "\n",
    "Problem: With more dimensions, the model can become overly complex and sensitive to noise in the training data.\n",
    "Impact: KNN might fit the training data too closely, capturing noise as if it were a significant pattern, which reduces its generalization ability to new, unseen data.\n",
    "Sparsity of Data:\n",
    "\n",
    "Problem: As the number of dimensions increases, the volume of the space increases exponentially, making the data points sparse.\n",
    "Impact: The sparsity means that data points are far from each other, and each point has fewer neighbors, reducing the effectiveness of the KNN algorithm which relies on local neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aeb77-9b57-4ae7-90b5-6166a0755725",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "Imputation Using KNN:\n",
    "KNN imputation replaces missing values by the mean (for continuous variables) or mode (for categorical variables) of the \n",
    "ùëò\n",
    "k-nearest neighbors.\n",
    "\n",
    "Steps:\n",
    "Identify Missing Values: Locate the missing values in the dataset.\n",
    "Calculate Distances: Compute distances between all pairs of data points using only the non-missing features.\n",
    "Find Neighbors: For each data point with a missing value, find its \n",
    "ùëò\n",
    "k-nearest neighbors based on the distances calculated.\n",
    "Impute Missing Values:\n",
    "For numerical features: Impute the missing value with the mean of the corresponding feature values from the \n",
    "ùëò\n",
    "k-nearest neighbors.\n",
    "For categorical features: Impute the missing value with the mode of the corresponding feature values from the \n",
    "ùëò\n",
    "k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9c4f3-1be4-4ef1-b20a-475e1043e9a3",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Best for problems requiring categorical predictions.\n",
    "Suitable for tasks like classification of emails, images, and diseases.\n",
    "Performance measured by metrics like accuracy, precision, and recall.\n",
    "\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Best for problems requiring continuous value predictions.\n",
    "Suitable for tasks like predicting prices, temperatures, and stock values.\n",
    "Performance measured by metrics like MSE, MAE, and \n",
    "ùëÖ\n",
    "2\n",
    "R \n",
    "2\n",
    "  score.\n",
    "The choice between KNN classifier and regressor depends on the nature of the target variable (categorical vs. continuous) and the specific requirements of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7335c54-6bfd-4719-bd07-10cbb5666eff",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "Classification:\n",
    "Simple and Intuitive: KNN is easy to understand and implement.\n",
    "Non-parametric: It does not make any assumptions about the underlying data distribution.\n",
    "Versatile: Can be used for both binary and multi-class classification problems.\n",
    "Adaptable: Works well with various types of distance metrics (e.g., Euclidean, Manhattan).\n",
    "Regression:\n",
    "Simple and Intuitive: Just like the classifier, KNN regressor is easy to understand and implement.\n",
    "Non-parametric: No assumptions about the data distribution are needed.\n",
    "Flexible: Can model complex relationships in data by considering local neighborhoods.\n",
    "Smooth Predictions: Provides smooth predictions based on the average of neighbors.\n",
    "Weaknesses of KNN\n",
    "Classification and Regression:\n",
    "Computational Complexity:\n",
    "\n",
    "Issue: High computational cost for distance calculations, especially with large datasets.\n",
    "Solution: Use efficient algorithms like KD-trees, Ball-trees, or approximate nearest neighbors (e.g., locality-sensitive hashing).\n",
    "Curse of Dimensionality:\n",
    "\n",
    "Issue: Performance degrades with high-dimensional data as distances become less meaningful.\n",
    "Solution: Apply dimensionality reduction techniques like PCA or feature selection to reduce the number of dimensions.\n",
    "Sensitivity to Noise and Irrelevant Features:\n",
    "\n",
    "Issue: KNN is sensitive to noisy data and irrelevant features, which can mislead distance calculations.\n",
    "Solution: Preprocess data by removing noise, normalizing/standardizing features, and selecting only relevant features.\n",
    "Imbalanced Data:\n",
    "\n",
    "Issue: KNN can be biased towards the majority class in imbalanced datasets.\n",
    "Solution: Use techniques like oversampling, undersampling, or adjusting class weights to handle imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9052b78-88dd-4462-96f2-96f920307c0d",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in K-Nearest Neighbors (KNN) algorithm. They measure the distance between two points in a multi-dimensional space, but they do so in different ways. Here are the differences between them:\n",
    "\n",
    "Euclidean Distance\n",
    "Definition:\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space.\n",
    "It is calculated as the square root of the sum of the squared differences between corresponding coordinates of the points.\n",
    "\n",
    "Definition:\n",
    "\n",
    "Manhattan distance, also known as Taxicab or L1 distance, is the sum of the absolute differences between corresponding coordinates of the points.\n",
    "It is called Manhattan distance because it mimics the way a taxi would travel in a grid-like city, like Manhattan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21adab-a490-4f11-87d7-771277b5a6b7",
   "metadata": {},
   "source": [
    "## 10\n",
    "\n",
    "Equal Contribution of Features:\n",
    "\n",
    "Issue: Features with larger scales can dominate the distance calculation, making the algorithm biased towards these features.\n",
    "Example: If one feature ranges from 0 to 1 (e.g., probability) and another ranges from 0 to 1000 (e.g., annual income), the larger range will disproportionately influence the distance metric.\n",
    "Solution: Scaling ensures that all features contribute equally to the distance calculation.\n",
    "Accuracy and Performance:\n",
    "\n",
    "Issue: Without scaling, KNN may give misleading results because the nearest neighbors might be determined by irrelevant large-scale features rather than the true underlying structure.\n",
    "Solution: Scaling features improves the accuracy and performance of the KNN algorithm by ensuring that all features are on a comparable scale.\n",
    "Distance Metrics:\n",
    "\n",
    "Issue: The choice of distance metric (e.g., Euclidean, Manhattan) is affected by the scale of features. Unscaled features can distort the distance calculations.\n",
    "Solution: Scaling features makes distance metrics more meaningful and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a8874-e8d7-4906-bd5e-89b5cf809bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
