{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5302e-d3d1-410f-91fc-e91c566d77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging (Bootstrap Aggregating): It involves training multiple instances of the same base learning\n",
    "algorithm on different subsets of the training data, and then averaging the predictions (for regression)\n",
    "or taking a vote (for classification).\n",
    "\n",
    "Boosting: It works by sequentially training models where each subsequent model corrects the errors made\n",
    "by the previous ones. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "Stacking: It combines the predictions of multiple base models (often of different types) using another\n",
    "model (called a meta-learner) to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b60e7-072a-49a9-881b-a6e4a90a1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Improved Accuracy: Ensembles can often achieve higher accuracy than individual models by reducing bias\n",
    "and variance, especially when the base models are diverse and complementary to each other.\n",
    "\n",
    "Robustness: Ensembles are more robust to overfitting, as the errors of individual models are often\n",
    "mitigated when combined. This can lead to better generalization to unseen data.\n",
    "\n",
    "Stability: Ensembles tend to be more stable and less sensitive to changes in the training data compared \n",
    "to individual models.\n",
    "\n",
    "Versatility: Ensemble techniques can be applied to a wide variety of machine learning tasks and algorithms,\n",
    "making them versatile tools in the machine learning toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa73eca-be23-4634-9d12-710e0d1bb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning. It involves\n",
    "training multiple instances of the same base learning algorithm on different subsets of the training data,\n",
    "sampled with replacement (bootstrap samples). Each model in the ensemble is trained independently, and the\n",
    "final prediction is typically made by averaging the predictions (for regression) or taking a vote\n",
    "(for classification) from all the models. Bagging helps to reduce overfitting and improve the stability\n",
    "and accuracy of the final model. Random Forest is a popular example of a bagging ensemble method, where \n",
    "the base learning algorithm is a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2926ff-b6c2-47e4-8551-91c696abddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble technique in machine learning that combines multiple weak learners \n",
    "(models that are slightly better than random guessing) to create a strong learner. Unlike bagging,\n",
    "which trains models independently, boosting trains models sequentially. Each subsequent model in the\n",
    "sequence focuses on correcting the errors made by the previous models.\n",
    "\n",
    "Boosting works by assigning weights to each training example. Initially, all examples are given equal\n",
    "weight. After each iteration, the weights of incorrectly classified examples are increased, so that \n",
    "subsequent models pay more attention to those examples. This process continues for a predefined number \n",
    "of iterations (or until a perfect model is achieved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc72259-e6eb-4e5b-9255-a1ef5b76ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness: Ensembles are more robust to overfitting, as the errors of individual models are often\n",
    "mitigated when combined. This can lead to better generalization to unseen data.\n",
    "\n",
    "Stability: Ensembles tend to be more stable and less sensitive to changes in the training data compared \n",
    "to individual models.\n",
    "\n",
    "Versatility: Ensemble techniques can be applied to a wide variety of machine learning tasks and algorithms,\n",
    "making them versatile tools in the machine learning toolbox.\n",
    "\n",
    "Handling Complex Relationships: By combining multiple models, ensembles can capture complex relationships\n",
    "in the data that may be difficult for individual models to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ebe4f-de99-451c-a1c7-1847497e6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble techniques are not always better than individual models. They can be computationally expensive, \n",
    "harder to interpret, and may amplify issues with low-quality or noisy data. In some cases, simpler models\n",
    "or improving data quality may be more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e8bc5-b522-4690-b6de-5a2a0cb9801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In bootstrap resampling, the confidence interval for a statistic (such as the mean or median) is \n",
    "calculated by repeatedly sampling with replacement from the observed data to create multiple bootstrap \n",
    "samples. For each bootstrap sample, the statistic of interest is computed. Then, the percentile method \n",
    "is commonly used to construct the confidence interval.\n",
    "\n",
    "Here a simplified step-by-step process:\n",
    "\n",
    "Bootstrap Sampling: Randomly sample with replacement from the observed data to create multiple bootstrap\n",
    "samples (typically thousands of times).\n",
    "\n",
    "Compute Statistic: Calculate the statistic of interest (e.g., mean, median, standard deviation) for each \n",
    "bootstrap sample.\n",
    "\n",
    "Construct Confidence Interval: Sort the computed statistic values from lowest to highest. Then, the \n",
    "confidence interval is determined by selecting the appropriate percentiles from this sorted list.\n",
    "\n",
    "For example, a 95% confidence interval would involve selecting the 2.5th and 97.5th percentiles. This\n",
    "means that 95% of the computed statistic values fall within this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67daffe-71af-4f0f-baa0-232ee5f4e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bootstrap is a statistical technique used to estimate how uncertain we are about a statistic \n",
    "(like the average or the difference between two groups) when we only have one sample of data.\n",
    "Here how it works:\n",
    "\n",
    "Create Samples: We start by making many new \"fake\" samples by randomly picking data points from our\n",
    "original sample, allowing some points to be picked multiple times.\n",
    "\n",
    "Calculate Statistic: For each fake sample, we calculate the statistic were interested in \n",
    "(e.g., the average).\n",
    "\n",
    "Estimate Uncertainty: By looking at how much the statistic varies across all the fake samples, we \n",
    "can estimate how uncertain we are about the true value of the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1115d2-ab04-4d87-8b56-45c125778e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Population Mean Height: [14.45169377 15.57761542]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sample_mean = 15  \n",
    "sample_std = 2     \n",
    "sample_size = 50 \n",
    "\n",
    "\n",
    "np.random.seed(20) \n",
    "n_bootstrap_samples = 10000\n",
    "bootstrap_means = []\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, sample_size)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "print(\"95% Confidence Interval for Population Mean Height:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d497479-e161-49e9-8995-55a316882888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
