{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd58a52c-8791-4850-932d-bdb52cd875f7",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Anomaly detection, also known as outlier detection, is a technique used in data mining and machine learning to identify unusual patterns or observations that do not conform to expected behavior within a dataset. The purpose of anomaly detection is to:\n",
    "\n",
    "Identify Unusual Patterns: Detect data points, events, or observations that deviate significantly from the majority of the data. These anomalies may represent rare events, errors, outliers, or novel patterns.\n",
    "\n",
    "Highlight Potential Issues: Flag anomalies that may indicate potential problems, anomalies, or opportunities for further investigation. For example, in cybersecurity, anomalies could indicate potential security breaches or attacks.\n",
    "\n",
    "Improve Data Quality: By identifying and handling anomalies, the overall quality and reliability of the data can be improved, leading to more accurate analysis and decision-making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdab4a-44dd-4077-9961-f8d8a5530e5b",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Anomaly detection is a powerful technique used across various domains, but it also comes with several key challenges that practitioners often need to address:\n",
    "\n",
    "Labeling Anomalies: Defining what constitutes an anomaly can be subjective and domain-specific. In some cases, anomalies are straightforward (e.g., fraudulent transactions), but in others, anomalies may be context-dependent and require expert knowledge to identify.\n",
    "\n",
    "Imbalanced Datasets: Anomalies are often rare compared to normal instances, leading to imbalanced datasets. This imbalance can affect the performance of anomaly detection algorithms, which may be biased towards the majority class (normal instances).\n",
    "\n",
    "High-Dimensional Data: With the rise of big data, datasets are becoming increasingly high-dimensional. Traditional anomaly detection methods may struggle with the curse of dimensionality, where the distance or density metrics lose effectiveness in high-dimensional spaces.\n",
    "\n",
    "Noise and Outliers: Distinguishing between anomalies and noise/outliers that do not represent meaningful anomalies can be challenging. Noise can obscure true anomalies and impact the performance of anomaly detection algorithms.\n",
    "\n",
    "Concept Drift: In dynamic environments, the concept of what constitutes normal behavior may change over time. Anomaly detection models trained on historical data may not generalize well to new data where the characteristics of normal and anomalous behavior have shifted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbb90a-1102-446c-aff3-84f2aa2e72bd",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "Data Requirement:\n",
    "\n",
    "No Labels: Unsupervised anomaly detection does not require labeled data explicitly indicating which instances are normal and which are anomalies. It relies solely on the characteristics of the data itself to identify outliers or patterns that deviate from the norm.\n",
    "Methodology:\n",
    "\n",
    "Statistical or Clustering Methods: Typically employs statistical methods (e.g., mean, variance, density estimation) or clustering algorithms (e.g., k-means, DBSCAN) to identify instances that are significantly different from the majority of the data points.\n",
    "Assumptions:\n",
    "\n",
    "Distribution Independence: Often assumes that anomalies are rare and significantly different from normal instances, without explicitly defining what constitutes normal behavior. This makes it suitable for detecting novel or unknown anomalies.\n",
    "Applications:\n",
    "\n",
    "Broad Applicability: Widely used in applications where labeled anomalies are scarce or unavailable, such as fraud detection, network security, and manufacturing quality control.\n",
    "Supervised Anomaly Detection:\n",
    "Data Requirement:\n",
    "\n",
    "Labeled Anomalies: Supervised anomaly detection requires a dataset with labeled instances indicating which are normal and which are anomalous. This labeled data is used to train a model that can classify new instances as normal or anomalous based on learned patterns.\n",
    "Methodology:\n",
    "\n",
    "Classification Models: Typically involves training supervised learning models, such as support vector machines (SVMs), decision trees, or neural networks, using labeled anomaly data to learn discriminative features and decision boundaries between normal and anomalous instances.\n",
    "Assumptions:\n",
    "\n",
    "Labeled Anomalies: Relies on the assumption that labeled anomalies accurately represent the types of anomalies expected in the data. Requires sufficient and representative labeled data for effective model training.\n",
    "Applications:\n",
    "\n",
    "Specific Use Cases: Useful in scenarios where labeled anomaly data is available and the types of anomalies to be detected are well-defined, such as medical diagnosis, credit card fraud detection, and defect detection in manufacturing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87389e-4c32-46c1-b09e-e73430896d60",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Statistical Methods:\n",
    "Description: Statistical methods assume that normal data points lie within a certain statistical distribution (e.g., Gaussian distribution), and anomalies are points that significantly deviate from this distribution.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Z-score: Measures the number of standard deviations a data point is from the mean.\n",
    "Q-statistics: Uses interquartile range to detect outliers.\n",
    "Kernel Density Estimation (KDE): Estimates the probability density function of the data and identifies outliers in low-density regions.\n",
    "2. Machine Learning-Based Methods:\n",
    "Description: Machine learning algorithms are trained to distinguish between normal and anomalous data points based on labeled or unlabeled datasets. Supervised methods require labeled anomaly data, while unsupervised methods do not.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Supervised Learning: Algorithms like Support Vector Machines (SVMs), Decision Trees, or Neural Networks trained with labeled data to classify anomalies.\n",
    "Unsupervised Learning: Clustering algorithms such as k-means, DBSCAN, or Isolation Forests, which identify anomalies as points that do not fit well into any cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62dc76-7ed9-4f07-82e9-20afab835289",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "Normal Data Concentration:\n",
    "\n",
    "Assumption: Normal data points are densely concentrated in the feature space, forming clusters or groups.\n",
    "Rationale: Anomalies are expected to be sparse and distant from these dense clusters of normal data points.\n",
    "Distance to Nearest Neighbors:\n",
    "\n",
    "Assumption: Anomalies have significantly larger distances to their nearest neighbors compared to normal data points.\n",
    "Rationale: Normal data points are expected to have similar characteristics and thus be closer to each other, whereas anomalies deviate significantly from the majority pattern.\n",
    "Distribution of Distances:\n",
    "\n",
    "Assumption: Distances between normal data points follow a certain distribution (e.g., Gaussian distribution).\n",
    "Rationale: Anomalies are expected to have distances that deviate from this distribution, either being much larger or smaller depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cdfff7-bcd1-4810-806d-7aca529cdf52",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the concept of local density deviation of a data point with respect to its neighbors. Here's a step-by-step overview of how LOF computes anomaly scores:\n",
    "\n",
    "Neighborhood Definition:\n",
    "\n",
    "For each data point \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    " , define its \n",
    "𝑘\n",
    "k-nearest neighbors. The parameter \n",
    "𝑘\n",
    "k is typically specified by the user or determined using heuristic methods like the distance to the \n",
    "𝑘\n",
    "k-th nearest neighbor.\n",
    "Reachability Distance:\n",
    "\n",
    "Compute the reachability distance \n",
    "reach_dist\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "reach_dist(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ) between \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  and each of its \n",
    "𝑘\n",
    "k-nearest neighbors \n",
    "𝑥\n",
    "𝑗\n",
    "x \n",
    "j\n",
    "​\n",
    " . The reachability distance is defined as:\n",
    "reach_dist\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "dist\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    ",\n",
    "dist_k\n",
    "(\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    ")\n",
    "reach_dist(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=max(dist(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ),dist_k(x \n",
    "j\n",
    "​\n",
    " ))\n",
    "\n",
    "where \n",
    "dist\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "dist(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ) is the Euclidean distance between \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  and \n",
    "𝑥\n",
    "𝑗\n",
    "x \n",
    "j\n",
    "​\n",
    " , and \n",
    "dist_k\n",
    "(\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "dist_k(x \n",
    "j\n",
    "​\n",
    " ) is the distance to the \n",
    "𝑘\n",
    "k-th nearest neighbor of \n",
    "𝑥\n",
    "𝑗\n",
    "x \n",
    "j\n",
    "​\n",
    " ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3be9a-dc08-4016-8728-d06009d8a852",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "Number of Trees (\n",
    "𝑛\n",
    "_\n",
    "𝑒\n",
    "𝑠\n",
    "𝑡\n",
    "𝑖\n",
    "𝑚\n",
    "𝑎\n",
    "𝑡\n",
    "𝑜\n",
    "𝑟\n",
    "𝑠\n",
    "n_estimators):\n",
    "\n",
    "Description: Determines the number of isolation trees to build. More trees can improve the algorithm's accuracy but increase computation time.\n",
    "Default Value: Typically set to 100, but can be adjusted based on the dataset size and complexity.\n",
    "Subsample Size (\n",
    "𝑚\n",
    "𝑎\n",
    "𝑥\n",
    "_\n",
    "𝑠\n",
    "𝑎\n",
    "𝑚\n",
    "𝑝\n",
    "𝑙\n",
    "𝑒\n",
    "𝑠\n",
    "max_samples):\n",
    "\n",
    "Description: Number of samples to draw from the dataset to build each isolation tree. A smaller sample size can speed up training but might reduce the algorithm's effectiveness, especially for large datasets.\n",
    "Default Value: Often set to 256, but can vary depending on the dataset size and characteristics.\n",
    "Contamination:\n",
    "\n",
    "Description: The expected proportion of anomalies in the dataset. It is used to adjust the threshold for anomaly detection.\n",
    "Default Value: Typically set to 'auto', which estimates the contamination based on the assumption that anomalies are rare. Alternatively, it can be set to a specific value reflecting the known or estimated proportion of anomalies in the dataset.\n",
    "Maximum Depth of Trees (\n",
    "𝑚\n",
    "𝑎\n",
    "𝑥\n",
    "_\n",
    "𝑑\n",
    "𝑒\n",
    "𝑝\n",
    "𝑡\n",
    "ℎ\n",
    "max_depth):\n",
    "\n",
    "Description: Maximum depth allowed for each isolation tree during construction. Controlling the maximum depth helps prevent overfitting and ensures the trees do not become overly complex.\n",
    "Default Value: Often set to 'None', allowing the trees to grow until all nodes are pure or contain fewer than the minimum number of samples required to split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0d02f-a588-499c-a828-58bc1fc0208a",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "Identify Neighbors:\n",
    "\n",
    "The data point has 2 neighbors within a radius of 0.5. Let's assume these are the nearest neighbors for simplicity.\n",
    "Assume K Neighbors:\n",
    "\n",
    "Since K=10, we consider the 10 nearest neighbors in total for a more generalized calculation.\n",
    "Calculate Reachability Distance:\n",
    "\n",
    "Compute the reachability distance between the data point \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  and each of its K nearest neighbors \n",
    "𝑥\n",
    "𝑗\n",
    "x \n",
    "j\n",
    "​\n",
    " .\n",
    "Local Reachability Density (LRD):\n",
    "\n",
    "Compute the LRD for the data point \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  using the reachability distances. LRD measures the inverse of the average reachability distance of \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  to its K nearest neighbors.\n",
    "Local Outlier Factor (LOF):\n",
    "\n",
    "Compute the LOF score for the data point \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  using the LRD values of its neighbors. LOF is calculated as the average ratio of the LRD of \n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  to the LRDs of its K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65445d50-ba51-4069-9b90-8f3a0afbf778",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "In the Isolation Forest algorithm, the anomaly score for a data point is based on its average path length (APL) compared to the average path length of the trees in the forest. Here’s how we can interpret and calculate the anomaly score given the information:\n",
    "\n",
    "Understanding Isolation Forest Anomaly Score:\n",
    "Average Path Length (APL):\n",
    "\n",
    "APL is the average number of edges traversed from the root to isolate a data point in each tree of the forest.\n",
    "A shorter APL indicates that the data point is easier to isolate and potentially more anomalous.\n",
    "Average Path Length of Trees:\n",
    "\n",
    "The average path length across all trees in the forest gives us a baseline. This baseline represents the average difficulty (in terms of path length) of isolating typical points in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e81635-96e7-4ac8-806f-4e68276592d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
