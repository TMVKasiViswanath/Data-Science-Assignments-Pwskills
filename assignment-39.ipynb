{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fec7ba-8ff3-49d3-939d-b8cf90a9032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV in machine learning is a technique used to tune hyperparameters by exhaustively searching \n",
    "through a specified parameter grid and cross-validating the results.\n",
    "\n",
    "How it works:\n",
    "\n",
    "GridSearchCV takes a dictionary of hyperparameters and their possible values as input.\n",
    "It performs an exhaustive search over all possible combinations of hyperparameters.\n",
    "For each combination, it trains the model using cross-validation.\n",
    "It evaluates the model performance using a scoring metric (e.g., accuracy, F1-score) and selects the best\n",
    "combination of hyperparameters based on this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852bf5e-33e1-4435-93ff-40ded4f596f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV:\n",
    "\n",
    "Exploration: GridSearchCV exhaustively searches through all possible combinations of hyperparameters.\n",
    "Computational Cost: It can be computationally expensive, especially with a large number of hyperparameters\n",
    "and their possible values.\n",
    "Use Case: GridSearchCV is suitable when you have a relatively small hyperparameter space and want to find\n",
    "the best combination of hyperparameters precisely.\n",
    "\n",
    "RandomizedSearchCV:\n",
    "\n",
    "Exploration: RandomizedSearchCV samples a fixed number of hyperparameter settings from the specified \n",
    "distributions.\n",
    "Computational Cost: It is less computationally expensive compared to GridSearchCV because it explores \n",
    "only a subset of the hyperparameter space.\n",
    "Use Case: RandomizedSearchCV is useful when the hyperparameter space is large and you want to balance\n",
    "computational cost with the quality of the hyperparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db701ac3-8956-491c-ba77-288063e1693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data leakage refers to the situation where information from outside the training\n",
    "dataset is used to create a model. This leads to overly optimistic performance estimates and unreliable \n",
    "models. Data leakage can occur at various stages of the machine learning pipeline, such as during data \n",
    "preprocessing, feature selection, or model evaluation.\n",
    "\n",
    "Example:\n",
    "Suppose you are building a model to predict whether a customer will default on a loan based on their\n",
    "financial history. If you accidentally include the loan status (defaulted or not) as a feature in the\n",
    "training data, the model will learn to rely heavily on this feature to make predictions. However, in a \n",
    "real-world scenario, this information would not be available at the time of prediction, leading to a\n",
    "model that performs poorly in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201459b1-e9c2-48f3-a153-4dceaab4950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Understand the data: Gain a deep understanding of the dataset and the problem you are trying to solve. \n",
    "Be aware of any potential sources of leakage, such as features that may contain information about the \n",
    "target variable that would not be available at prediction time.\n",
    "\n",
    "Split the data properly: Use a proper splitting strategy \n",
    "(e.g., train/validation/test split or cross-validation) to ensure that no data from the validation or test \n",
    "set leaks into the training set.\n",
    "\n",
    "Preprocess data carefully: Be mindful of preprocessing steps that could introduce leakage.\n",
    "For example, normalizing the entire dataset before splitting it can lead to data leakage, as the \n",
    "normalization parameters should be calculated only on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656f24c-9b43-4034-924d-90fbbcac7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A confusion matrix is a table that shows how well a classification model is performing.\n",
    "It has rows for the actual classes and columns for the predicted classes. The main diagonal\n",
    "shows correct predictions, while off-diagonal elements show errors. It helps you understand\n",
    "where the model is making mistakes and how well it's doing overall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a133b3-dc3b-4e91-a732-45686012e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision (also called positive predictive value) measures the accuracy of the\n",
    "positive predictions made by the model. It is calculated as the number of true positive predictions\n",
    "divided by the total number of positive predictions made by the model\n",
    "(i.e., true positives plus false positives). Precision focuses on the quality of the positive predictions.\n",
    "\n",
    "Recall (also called sensitivity or true positive rate) measures the proportion of actual positive \n",
    "instances that were correctly identified by the model. It is calculated as the number of true positive \n",
    "predictions divided by the total number of actual positive instances \n",
    "(i.e., true positives plus false negatives). Recall focuses on the model ability to find all\n",
    "the positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a9cda-962a-4047-b794-8eaf43ce3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "True Positives (TP): These are cases where the model correctly predicted the positive class.\n",
    "For example, in a medical context, these would be cases where the model correctly identified a patient\n",
    "with a disease.\n",
    "\n",
    "False Positives (FP): These are cases where the model incorrectly predicted the positive class. In the \n",
    "medical example, this would be a case where the model predicted a patient had a disease when they did not.\n",
    "\n",
    "True Negatives (TN): These are cases where the model correctly predicted the negative class. Using the\n",
    "medical example, this would be a case where the model correctly predicted that a patient did not have a\n",
    "disease.\n",
    "\n",
    "False Negatives (FN): These are cases where the model incorrectly predicted the negative class. \n",
    "In the medical example, this would be a case where the model predicted that a patient did not have\n",
    "a disease when they actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555afc9-2d68-4138-b620-a7da6d92131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy:\n",
    "\n",
    "Accuracy measures the proportion of correct predictions among all predictions made by the model.\n",
    "Formula: (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision:\n",
    "\n",
    "Precision measures the proportion of true positive predictions among all positive predictions made by the \n",
    "model.\n",
    "Formula: TP / (TP + FP)\n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "Recall measures the proportion of true positive predictions among all actual positive instances.\n",
    "Formula: TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9087f-0b0a-4b2a-baf0-c6b98556c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "The accuracy of a model is directly related to the values in its confusion matrix, specifically the True \n",
    "Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "Accuracy is calculated as the proportion of correct predictions (TP + TN) out of all predictions made by\n",
    "the model (TP + TN + FP + FN).\n",
    "\n",
    "High accuracy: A high accuracy indicates that the model is making a high proportion of correct predictions\n",
    "relative to all predictions made. This means that the values of TP and TN are relatively high compared to\n",
    "FP and FN.\n",
    "\n",
    "Low accuracy: A low accuracy indicates that the model is making a high proportion of incorrect predictions\n",
    "relative to all predictions made. This could be due to a high number of FP or FN, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd394b79-f8d8-44a1-8452-68a116255d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class Imbalance: Check if your model predicts one class much more than others. \n",
    "This could mean it's biased towards the majority class.\n",
    "\n",
    "Misclassification Patterns: Look for patterns where your model consistently predicts one class as another.\n",
    "This could show that your model needs better features to tell those classes apart.\n",
    "\n",
    "Precision and Recall: Check if some classes have low recall or precision. Low recall means the model\n",
    "misses many instances of that class, while low precision means it wrongly predicts that class often.\n",
    "\n",
    "False Positives and False Negatives: See if your model has high rates of false positives or false\n",
    "negatives. This can show where the model struggles and needs improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
