{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d857a8-dc5b-420c-9c08-48a5ba29f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by training multiple trees on\n",
    "different bootstrap samples of the training data and then averaging their predictions. Here's how it works:\n",
    "\n",
    "Bootstrap Sampling: Bagging creates multiple bootstrap samples (random samples with replacement) from \n",
    "the original training data. Each bootstrap sample is used to train a separate decision tree.\n",
    "\n",
    "Independent Training: Each decision tree is trained independently on a different bootstrap sample. This\n",
    "means that each tree is exposed to slightly different subsets of the training data.\n",
    "\n",
    "Reduced Variance: Because each tree is trained on a different subset of the data, they are likely to \n",
    "make different errors. By averaging the predictions of all the trees, the errors tend to cancel out,\n",
    "reducing the overall variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d646c7-ae3c-4439-853c-37add9acc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages:\n",
    "\n",
    "Diversity: Different types of base learners can capture different aspects of the data and make different \n",
    "types of errors. This diversity can lead to a more robust and accurate ensemble.\n",
    "\n",
    "Complementary Strengths: Each base learner may have its strengths and weaknesses. By combining them, the\n",
    "ensemble can benefit from the strengths of each base learner while mitigating their individual weaknesses.\n",
    "\n",
    "Reduced Overfitting: If the base learners are diverse enough, they are less likely to overfit the training\n",
    "data. This can lead to better generalization to unseen data.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Using different types of base learners can make the overall model more complex and harder \n",
    "to interpret.\n",
    "\n",
    "Implementation Complexity: Implementing and managing multiple types of base learners can be more \n",
    "challenging than using a single type.\n",
    "\n",
    "Training Time: Training different types of base learners can be more time-consuming than training a\n",
    "single type, especially if the base learners are complex or require different preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5962c4f-4cbd-4a96-9042-b66df0093891",
   "metadata": {},
   "outputs": [],
   "source": [
    "Low Bias, High Variance Base Learners: If the base learner has low bias but high variance \n",
    "(e.g., deep decision trees), bagging can significantly reduce the variance by averaging the predictions \n",
    "of multiple trees trained on different subsets of the data. This can lead to a reduction in overfitting\n",
    "and better generalization to unseen data.\n",
    "\n",
    "High Bias, Low Variance Base Learners: If the base learner has high bias but low variance\n",
    "(e.g., shallow decision trees), bagging may not have as much of an impact on the bias-variance tradeoff.\n",
    "While bagging can still reduce variance to some extent, the base learner high bias may limit the overall \n",
    "performance of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae5f7b-4895-46a4-b6b5-69ccaaea9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, bagging can be used for both classification and regression tasks. In both cases, it involves \n",
    "training multiple models on different subsets of the data and then combining their predictions.\n",
    "\n",
    "For regression tasks (predicting continuous values like house prices), the final prediction is often the\n",
    "average of the predictions from all the models.\n",
    "\n",
    "For classification tasks (predicting categories like spam or not spam), the final prediction is usually \n",
    "the most commonly predicted class (majority vote) or the class with the highest average probability from\n",
    "the models.\n",
    "\n",
    "The main difference is in how the final prediction is made and how the models are evaluated, but the basic\n",
    "idea of bagging is the same for both types of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee98d38-72f9-4b6b-83fb-2a7bb7b8d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ensemble size in bagging refers to the number of base models (e.g., decision trees) that are trained \n",
    "on different subsets of the data and whose predictions are combined to make the final prediction. The\n",
    "role of ensemble size is to balance the trade-off between model performance and computational resources.\n",
    "\n",
    "Generally, increasing the ensemble size can lead to better performance up to a certain point, as it allows\n",
    "for more diverse base models and can reduce the variance of the ensemble. However, after a certain point,\n",
    "increasing the ensemble size may result in diminishing returns or even overfitting, especially if the base\n",
    "models are too complex or if there is not enough diversity among them.\n",
    "\n",
    "The optimal ensemble size depends on several factors, including the complexity of the base models, the\n",
    "size and diversity of the dataset, and the computational resources available. It is often recommended \n",
    "to start with a moderate ensemble size (e.g., 50-500 models) and then use cross-validation or other \n",
    "techniques to determine if increasing the ensemble size further improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5879889-5482-4f61-a44e-90526996325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "One real-world application of bagging in machine learning is in the field of finance for predicting stock \n",
    "prices.\n",
    "\n",
    "In this application, multiple base models, such as decision trees or random forests, are trained on\n",
    "historical stock price data and other relevant features. Each base model makes its own prediction of the \n",
    "future stock price movement.\n",
    "\n",
    "By using bagging to combine the predictions of these base models, the final ensemble model can provide \n",
    "a more accurate prediction of the stock price movement. This approach helps to reduce the impact of \n",
    "individual base models errors and can improve the overall prediction accuracy.\n",
    "\n",
    "Bagging is particularly useful in this context because stock price movements can be influenced by a wide\n",
    "range of factors, and using an ensemble of models helps to capture this complexity and make more robust\n",
    "predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
