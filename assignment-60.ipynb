{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e32ffe4-a588-43a8-830b-ea24c1671752",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "\n",
    "Hierarchical clustering is a method of cluster analysis which seeks to build a hierarchy of clusters. Unlike partition-based methods such as K-means, hierarchical clustering does not require specifying the number of clusters in advance and produces a dendrogram, a tree-like diagram that records the sequences of merges or splits\n",
    "\n",
    "Differences from Other Clustering Techniques\n",
    "K-means Clustering:\n",
    "\n",
    "Cluster Structure: K-means produces a flat partition of clusters without any hierarchy, whereas hierarchical clustering produces a dendrogram.\n",
    "Number of Clusters: K-means requires the number of clusters to be specified beforehand. Hierarchical clustering does not require this; clusters can be formed by cutting the dendrogram at the desired level.\n",
    "Distance Metric: K-means uses Euclidean distance by default, while hierarchical clustering can use various distance metrics and linkage criteria.\n",
    "Centroids vs. Linkages: K-means assigns points to the nearest centroid, whereas hierarchical clustering uses linkages to determine the closeness of clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49311171-7a4a-4ad0-a799-f3ddf5ec7a1b",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Types of Hierarchical Clustering\n",
    "Agglomerative (Bottom-Up) Clustering:\n",
    "\n",
    "Process:\n",
    "Start with each data point as its own cluster.\n",
    "Iteratively merge the closest pairs of clusters.\n",
    "Continue until all points are in a single cluster or a desired number of clusters is reached.\n",
    "Characteristics:\n",
    "The process is represented as a dendrogram, which can be cut at a desired level to obtain a specific number of clusters.\n",
    "Common linkage criteria include single linkage (minimum distance), complete linkage (maximum distance), average linkage, and Ward's method (minimizes variance).\n",
    "Divisive (Top-Down) Clustering:\n",
    "\n",
    "Process:\n",
    "Start with all data points in a single cluster.\n",
    "Iteratively split clusters into smaller clusters.\n",
    "Continue until each data point is in its own cluster or a desired number of clusters is reached.\n",
    "Characteristics:\n",
    "Less commonly used due to higher computational cost compared to agglomerative clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97e56a-593c-4014-8a0b-d79e500cf8f1",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "1. Distance Between Clusters (Linkage Criteria)\n",
    "The distance between two clusters \n",
    "𝐶\n",
    "𝑖\n",
    "C \n",
    "i\n",
    "​\n",
    "  and \n",
    "𝐶\n",
    "𝑗\n",
    "C \n",
    "j\n",
    "​\n",
    "  is typically based on the distances between their member data points. Several common linkage criteria or distance metrics are used to compute this distance:\n",
    "\n",
    "2. Common Distance Metrics (Linkage Criteria)\n",
    "Single Linkage (Minimum Linkage):\n",
    "\n",
    "Definition: Measures the shortest distance between clusters based on the closest pair of points (one from each cluster).\n",
    "Formula: \n",
    "𝑑\n",
    "(\n",
    "𝐶\n",
    "𝑖\n",
    ",\n",
    "𝐶\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "min\n",
    "⁡\n",
    "𝑥\n",
    "∈\n",
    "𝐶\n",
    "𝑖\n",
    ",\n",
    "𝑦\n",
    "∈\n",
    "𝐶\n",
    "𝑗\n",
    "dist\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "d(C \n",
    "i\n",
    "​\n",
    " ,C \n",
    "j\n",
    "​\n",
    " )=min \n",
    "x∈C \n",
    "i\n",
    "​\n",
    " ,y∈C \n",
    "j\n",
    "​\n",
    " \n",
    "​\n",
    " dist(x,y)\n",
    "Characteristics: Tends to form elongated clusters and is sensitive to outliers and noise.\n",
    "Complete Linkage (Maximum Linkage):\n",
    "\n",
    "Definition: Measures the longest distance between clusters based on the farthest pair of points (one from each cluster).\n",
    "Formula: \n",
    "𝑑\n",
    "(\n",
    "𝐶\n",
    "𝑖\n",
    ",\n",
    "𝐶\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "𝑥\n",
    "∈\n",
    "𝐶\n",
    "𝑖\n",
    ",\n",
    "𝑦\n",
    "∈\n",
    "𝐶\n",
    "𝑗\n",
    "dist\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "d(C \n",
    "i\n",
    "​\n",
    " ,C \n",
    "j\n",
    "​\n",
    " )=max \n",
    "x∈C \n",
    "i\n",
    "​\n",
    " ,y∈C \n",
    "j\n",
    "​\n",
    " \n",
    "​\n",
    " dist(x,y)\n",
    "Characteristics: Produces more compact clusters and is less sensitive to outliers compared to single linkage.\n",
    "Average Linkage:\n",
    "\n",
    "Definition: Measures the average distance between all pairs of points (one from each cluster).\n",
    "Formula: \n",
    "𝑑\n",
    "(\n",
    "𝐶\n",
    "𝑖\n",
    ",\n",
    "𝐶\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "1\n",
    "∣\n",
    "𝐶\n",
    "𝑖\n",
    "∣\n",
    "⋅\n",
    "∣\n",
    "𝐶\n",
    "𝑗\n",
    "∣\n",
    "∑\n",
    "𝑥\n",
    "∈\n",
    "𝐶\n",
    "𝑖\n",
    ",\n",
    "𝑦\n",
    "∈\n",
    "𝐶\n",
    "𝑗\n",
    "dist\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "d(C \n",
    "i\n",
    "​\n",
    " ,C \n",
    "j\n",
    "​\n",
    " )= \n",
    "∣C \n",
    "i\n",
    "​\n",
    " ∣⋅∣C \n",
    "j\n",
    "​\n",
    " ∣\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "x∈C \n",
    "i\n",
    "​\n",
    " ,y∈C \n",
    "j\n",
    "​\n",
    " \n",
    "​\n",
    " dist(x,y)\n",
    "Characteristics: Balances between single and complete linkage, providing a compromise between cluster compactness and sensitivity to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f14042-0742-43fa-b110-835fcc28084d",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Determining the optimal number of clusters in hierarchical clustering can be approached in several ways, leveraging the dendrogram and other metrics. Here are some common methods used for determining the optimal number of clusters:\n",
    "\n",
    "1. Inspecting the Dendrogram:\n",
    "Method: Visual examination of the dendrogram.\n",
    "Explanation: The dendrogram visually displays how clusters are merged as you move up from the leaves to the root. The optimal number of clusters can often be identified by looking for the largest vertical distance that doesn't have a horizontal line passing through it. This distance indicates the greatest dissimilarity at which clusters are merged.\n",
    "2. Using the Elbow Method:\n",
    "Method: Analyzing the rate of change of within-cluster dissimilarities.\n",
    "Explanation: Calculate the dissimilarity measure (e.g., distance) at each merge step and plot it against the number of clusters. Look for a point where the rate of change sharply decreases (forming an \"elbow\"), suggesting that further merging provides diminishing returns in terms of reducing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060406c-6905-4eb5-9a72-61c94c3e5bff",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "Dendrograms in hierarchical clustering are tree-like structures that visually represent the merging (agglomerative) or splitting (divisive) of clusters at each step of the clustering process. They are fundamental to hierarchical clustering as they provide a detailed and intuitive way to understand the relationships and hierarchy among clusters and data points. Here’s how dendrograms are structured and why they are useful in analyzing clustering results:\n",
    "\n",
    "Structure of Dendrograms:\n",
    "Vertical Axis: Represents the distance or dissimilarity between clusters or data points. The height of each fusion (or split) in the dendrogram indicates how dissimilar (or similar) the clusters being merged (or split) are.\n",
    "\n",
    "Horizontal Axis: Represents individual data points or clusters. Each data point starts as its own cluster, and clusters are progressively merged (agglomerative clustering) or split (divisive clustering) as you move from left to right along the horizontal axis.\n",
    "\n",
    "Branches and Nodes: Connections between branches (or nodes) indicate where clusters merge or split. The length of these connections typically represents the distance or dissimilarity at which the merge or split occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e45698-d306-4011-b636-1e8fa00228e5",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "Yes, hierarchical clustering can be used for both numerical and categorical data. However, the choice of distance metrics or similarity measures differs depending on the type of data being clustered:\n",
    "\n",
    "Numerical Data:\n",
    "For numerical data, distance metrics typically measure the distance between data points in a continuous space. Common distance metrics used include:\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Definition: Measures the straight-line distance between two points in a Euclidean space.\n",
    "Formula: \n",
    "𝑑\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "d(x,y)= \n",
    "∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " −y \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " \n",
    "Characteristics: Suitable for data where the magnitude and scale of differences between numerical attributes matter.\n",
    "Manhattan Distance (City Block Distance):\n",
    "\n",
    "Definition: Measures the sum of absolute differences between corresponding attributes.\n",
    "Formula: \n",
    "𝑑\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝑥\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "𝑖\n",
    "∣\n",
    "d(x,y)=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣x \n",
    "i\n",
    "​\n",
    " −y \n",
    "i\n",
    "​\n",
    " ∣\n",
    "Characteristics: Useful when the data attributes are measured in different units or scales.\n",
    "Cosine Similarity:\n",
    "\n",
    "Definition: Measures the cosine of the angle between two vectors in a multi-dimensional space.\n",
    "Formula: \n",
    "similarity\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "=\n",
    "𝑥\n",
    "⋅\n",
    "𝑦\n",
    "∥\n",
    "𝑥\n",
    "∥\n",
    "∥\n",
    "𝑦\n",
    "∥\n",
    "similarity(x,y)= \n",
    "∥x∥∥y∥\n",
    "x⋅y\n",
    "​\n",
    " \n",
    "Characteristics: Suitable for high-dimensional data where the magnitude of vectors is more important than their absolute values.\n",
    "Categorical Data:\n",
    "For categorical data, different distance metrics that account for the discrete nature of attributes are used:\n",
    "\n",
    "Hamming Distance:\n",
    "\n",
    "Definition: Measures the number of positions at which corresponding elements are different between two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76df9bd-e8e1-4e97-87ce-dd10c2cd5852",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "Perform Hierarchical Clustering:\n",
    "\n",
    "Apply hierarchical clustering to your dataset using an appropriate distance metric (e.g., Euclidean distance for numerical data, Hamming distance for categorical data).\n",
    "Choose a linkage method (e.g., single linkage, complete linkage) that suits your data and clustering objectives.\n",
    "Construct the Dendrogram:\n",
    "\n",
    "Visualize the resulting dendrogram to understand how clusters are formed and how data points are grouped together.\n",
    "The dendrogram provides insights into the hierarchical structure of clusters and helps in identifying outliers based on their distance from other clusters.\n",
    "Identify Outlier Branches or Singletons:\n",
    "\n",
    "Look for branches in the dendrogram that are isolated or have a large height (distance) compared to others.\n",
    "Points that do not merge with any other cluster until a high distance in the dendrogram can indicate potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f89a67-00ec-4905-b65e-dd50cd31d46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
