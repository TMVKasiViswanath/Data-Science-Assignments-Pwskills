{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d33337-5295-49d1-882d-f261065dba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling is a data preprocessing technique used to scale numerical features to a fixed range, \n",
    "usually between 0 and 1. It is done by subtracting the minimum value of the feature and then dividing \n",
    "by the range of the feature (the maximum value minus the minimum value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ccd1f3-eedc-420a-99e9-d6973a939d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   age_of_houses  size_of_houses\n",
      "0             20            1000\n",
      "1             50            3000\n",
      "2             80            5000\n",
      "\n",
      "Scaled Data:\n",
      "   age_of_houses  size_of_houses\n",
      "0            0.0             0.0\n",
      "1            0.5             0.5\n",
      "2            1.0             1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'age_of_houses': [20, 50, 80],\n",
    "    'size_of_houses': [1000, 3000, 5000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0847335-6c93-4cb2-9a41-01cf02508424",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique in feature scaling, also known as normalization, scales each feature so that \n",
    "the magnitude of each feature vector is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac12ad0-d4f5-4f43-b6ac-6a3755f5c4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   x  y\n",
      "0  2  1\n",
      "1  4  3\n",
      "2  6  5\n",
      "\n",
      "Normalized Data:\n",
      "          x         y\n",
      "0  0.894427  0.447214\n",
      "1  0.800000  0.600000\n",
      "2  0.768221  0.640184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    'x': [2, 4, 6],\n",
    "    'y': [1, 3, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "normalizer = Normalizer(norm='l2')  \n",
    "\n",
    "\n",
    "normalized_data = normalizer.fit_transform(df)\n",
    "\n",
    "\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(normalized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db509b3-89d4-49b6-a45e-8a7069522a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to reduce the number of \n",
    "features (or dimensions) in a dataset while retaining as much variance as possible. It does this by \n",
    "transforming the original features into a new set of orthogonal (uncorrelated) features called principal \n",
    "components. These principal components are ordered by the amount of variance they explain in the data, \n",
    "with the first component explaining the most variance.\n",
    "\n",
    "PCA is used in dimensionality reduction to:\n",
    "\n",
    "Reduce Overfitting: By reducing the number of features, PCA can help reduce overfitting in machine learning\n",
    "models.\n",
    "Improve Model Performance: PCA can improve the performance of machine learning models by focusing on the \n",
    "most important features.\n",
    "Visualize High-dimensional Data: PCA can be used to visualize high-dimensional data in a lower-dimensional \n",
    "space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede4a719-c3b5-4723-a9ee-777e78c996ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    x1   x2   x3\n",
      "0  1.0  2.0  3.0\n",
      "1  4.0  5.0  6.0\n",
      "2  7.0  8.0  9.0\n",
      "\n",
      "PCA-transformed Data:\n",
      "        PC1           PC2\n",
      "0 -5.196152  2.563950e-16\n",
      "1  0.000000  0.000000e+00\n",
      "2  5.196152  2.563950e-16\n",
      "\n",
      "Explained Variance Ratio:\n",
      "[1.00000000e+00 2.43475588e-33]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    'x1': [1.0, 4.0, 7.0],\n",
    "    'x2': [2.0, 5.0, 8.0],\n",
    "    'x3': [3.0, 6.0, 9.0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_data = pca.fit_transform(df)\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nPCA-transformed Data:\")\n",
    "print(pca_df)\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfaf24-04a6-4779-803e-6e81593ea3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA can be used for feature extraction by transforming the original features into a new set of features \n",
    "(principal components) that capture the most important information in the data. This reduces the \n",
    "dimensionality of the dataset while retaining as much variance as possible, making it useful for \n",
    "reducing overfitting and improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f263a0d4-4353-4244-b976-b669fad87c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   feature1  feature2  feature3  feature4\n",
      "0         1         4         1         2\n",
      "1         2         3         3         4\n",
      "2         3         2         2         1\n",
      "3         4         1         4         3\n",
      "\n",
      "Extracted Features (PC1 and PC2):\n",
      "            PC1       PC2\n",
      "0  2.645751e+00 -0.000000\n",
      "1 -5.192593e-16  1.732051\n",
      "2  3.115556e-16 -1.732051\n",
      "3 -2.645751e+00 -0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4],\n",
    "    'feature2': [4, 3, 2, 1],\n",
    "    'feature3': [1, 3, 2, 4],\n",
    "    'feature4': [2, 4, 1, 3]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "extracted_features = pca.fit_transform(df)\n",
    "\n",
    "extracted_df = pd.DataFrame(data=extracted_features, columns=['PC1', 'PC2'])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nExtracted Features (PC1 and PC2):\")\n",
    "print(extracted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86fe76-26a1-469c-b39d-e015991d970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Understand the Dataset: Start by understanding the dataset and the features it\n",
    "contains, such as price, rating, and delivery time.\n",
    "\n",
    "Apply Min-Max Scaling: For each feature (price, rating, delivery time), apply Min-Max scaling to \n",
    "scale the values to a range between 0 and 1. This ensures that all features are on a similar scale,\n",
    "which is important for many machine learning algorithms.\n",
    "\n",
    "Interpretation: After scaling the data, the features (price, rating, delivery time) will be transformed\n",
    "to a range between 0 and 1. This makes it easier to compare and analyze the features in the context of the\n",
    "recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c15106-0d80-47a1-bd04-f365a74201fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Standardize the Data: Ensure that each feature has a mean of 0 and a standard deviation of 1 to make all \n",
    "features contribute equally to the principal components.\n",
    "\n",
    "Apply PCA: Transform the standardized data into its principal components, which identify the directions \n",
    "along which the data varies the most.\n",
    "\n",
    "Select the Number of Components: Choose the number of principal components to retain based on the explained\n",
    "variance ratio, indicating the proportion of variance in the original data explained by each component.\n",
    "\n",
    "Project the Data: Project the original data onto the selected principal components to obtain the \n",
    "reduced-dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d306d1-a099-4415-bc9f-304f8318635e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "min_val = -1\n",
    "max_val = 1\n",
    "min_data = np.min(data)\n",
    "max_data = np.max(data)\n",
    "scaled_data = ((data - min_data) / (max_data - min_data)) * (max_val - min_val) + min_val\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c557d3-0d37-42af-8158-acfe2e5e7fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principal Components to Retain: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    'height': [170, 180, 165, 175, 160],\n",
    "    'weight': [70, 80, 60, 75, 55],\n",
    "    'age': [30, 35, 25, 40, 22],\n",
    "    'gender': ['male', 'male', 'female', 'female', 'female'],\n",
    "    'blood_pressure': [120, 130, 110, 125, 105]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "df['gender'] = df['gender'].map({'male': 0, 'female': 1})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(df[['height', 'weight', 'age', 'gender', 'blood_pressure']])\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "total_variance = sum(pca.explained_variance_ratio_)\n",
    "for i, explained_variance in enumerate(pca.explained_variance_ratio_):\n",
    "    if sum(pca.explained_variance_ratio_[:i+1]) >= 0.95 * total_variance:\n",
    "        n_components = i+1\n",
    "        break\n",
    "\n",
    "print(\"Number of Principal Components to Retain:\", n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a91c50-7dd8-4aa7-ba7e-a99911074230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87969c-ea64-4743-ac27-b1e110771644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
