{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cd7fd-da2e-49c3-bdd8-39fa45d40162",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting: Model learns noise in the training data, leading to poor performance\n",
    "on new data. Mitigation: Cross-validation, regularization, feature selection, early stopping, ensemble \n",
    "methods.\n",
    "\n",
    "Underfitting: Model is too simple to capture underlying patterns. Poor performance on both training and\n",
    "new data. Mitigation: Increase model complexity, add more features, reduce regularization, use a different\n",
    "algorithm.\n",
    "\n",
    "Cross-validation: Use techniques like k-fold cross-validation to evaluate the models performance on\n",
    "multiple splits of the data.\n",
    "Regularization: Add a penalty term to the models objective function to discourage overly complex models. \n",
    "Common regularization techniques include L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "Feature selection: Select only the most relevant features to reduce the models complexity and improve\n",
    "generalization.\n",
    "Early stopping: Stop the training process when the performance on a validation set starts to degrade, \n",
    "thus preventing the model from overfitting.\n",
    "Ensemble methods: Combine multiple models to reduce overfitting. Examples include bagging \n",
    "(e.g., Random Forests) and boosting (e.g., AdaBoost).\n",
    "\n",
    "\n",
    "To mitigate underfitting, you can:\n",
    "    \n",
    "Increase model complexity: Use a more complex model that can capture the underlying patterns in the data.\n",
    "Add more features: If the model is too simple, adding more relevant features can help it better capture \n",
    "the underlying patterns.\n",
    "Reduce regularization: If the model is being penalized too much for complexity, reducing the regularization\n",
    "strength can help.\n",
    "Use a different algorithm: Sometimes, switching to a different algorithm that is better suited to the \n",
    "data can help reduce underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bc2b3-2d05-48f6-bd06-006828f84fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-validation: Use techniques like k-fold cross-validation to assess model \n",
    "performance on multiple data subsets. This helps evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "Regularization: Introduce penalties on model parameters to discourage overly complex models. \n",
    "Techniques like L1 regularization (Lasso) and L2 regularization (Ridge) help prevent overfitting by \n",
    "constraining parameter values.\n",
    "\n",
    "Feature selection: Choose only the most relevant features to train the model. Removing irrelevant or \n",
    "redundant features reduces model complexity and helps focus on important patterns in the data.\n",
    "\n",
    "Early stopping: Monitor model performance on a validation set during training and stop training when \n",
    "performance begins to degrade. This prevents the model from continuing to learn noise in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccfa42-cf7a-4794-bc51-91f997fe1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the\n",
    "data, resulting in poor performance on both the training and test datasets. This can happen due to various \n",
    "easons, including:\n",
    "\n",
    "Model Complexity: Using a model that is too simple, such as a linear model for data with non-linear\n",
    "relationships, can lead to underfitting.\n",
    "\n",
    "Insufficient Features: If important features are missing from the dataset, the model may not have enough \n",
    "information to make accurate predictions.\n",
    "\n",
    "Over-regularization: Applying too much regularization can constrain the model too much, making it too \n",
    "simple to capture the underlying patterns in the data.\n",
    "\n",
    "Small Training Dataset: With a small dataset, the model may not have enough examples to learn the \n",
    "underlying patterns, leading to underfitting.\n",
    "\n",
    "Noisy Data: If the data contains a lot of noise or irrelevant information, the model may struggle to learn\n",
    "the true underlying patterns.\n",
    "\n",
    "Inappropriate Algorithm: Using an algorithm that is not suitable for the dataset or problem at hand can\n",
    "lead to underfitting. For example, using a linear regression model for a highly non-linear dataset.\n",
    "\n",
    "Bias in the Data: If the data is biased or unrepresentative of the true underlying distribution, the model\n",
    "may underfit by failing to capture the true patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57c06d-7704-40eb-b76a-86fade0ae53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The bias-variance tradeoff is about finding the right balance between two types of errors that affect \n",
    "how well a machine learning model can make predictions.\n",
    "\n",
    "using very simple models to the very complex data sets leads to the higher bias\n",
    "but in varience the model is very much sensitive to the training model and do not make good predictions for\n",
    "the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3bd9f-5a2f-469f-9c10-b9870b6b9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Validation Curves: Plot the model training and validation performance against varying model complexity \n",
    "(e.g., degree of polynomial features). Look for the point where the validation error starts to increase\n",
    "while the training error continues to decrease, indicating overfitting.\n",
    "\n",
    "Learning Curves: Plot the model performance (e.g., accuracy or error) against the size of the training \n",
    "dataset. An underfit model will have high error on both training and validation sets that does not \n",
    "decrease with more data, while an overfit model will have a large gap between the two curves.\n",
    "\n",
    "Cross-validation: Perform k-fold cross-validation to evaluate the model performance on different subsets \n",
    "of the data. If the model performs significantly worse on the validation sets compared to the training sets,\n",
    "it may be overfitting.\n",
    "\n",
    "Feature Importance: If your model has features with non-zero coefficients (e.g., in linear models) or \n",
    "feature importances (e.g., in tree-based models), check if any of these features are dominating the model\n",
    "predictions. Removing less important features may help reduce overfitting.\n",
    "\n",
    "Regularization: If your model supports regularization (e.g., L1 or L2 regularization in linear models),\n",
    "try increasing the regularization strength to reduce overfitting.\n",
    "\n",
    "Determining Overfitting or Underfitting:\n",
    "\n",
    "Training Error vs. Validation Error: If the training error is much lower than the validation error, the\n",
    "model is likely overfitting. If both errors are high, the model may be underfitting.\n",
    "\n",
    "Model Complexity: If a simpler model (e.g., lower-degree polynomial) performs better than a more complex\n",
    "model (e.g., higher-degree polynomial), the complex model may be overfitting.\n",
    "\n",
    "Visual Inspection: Plotting the models predictions against the true values can provide visual clues.\n",
    "A model that fits the training data too closely (with lots of wiggles) may be overfitting, while a model\n",
    "that is too simple may be underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da8729f-a44e-4405-8b89-8633812777db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias:\n",
    "\n",
    "Definition: Bias is the error introduced by approximating a real-world problem, which may be complex, by a \n",
    "much simpler model.\n",
    "Characteristics: High bias models are typically too simple and make strong assumptions about the form of\n",
    "the underlying function. They may underfit the training data.\n",
    "Impact: High bias models may have low accuracy on both the training and test datasets.\n",
    "Examples: Linear regression, logistic regression with a linear decision boundary.\n",
    "Variance:\n",
    "\n",
    "Definition: Variance is the error introduced by the model sensitivity to small fluctuations in the training\n",
    "data.\n",
    "Characteristics: High variance models are overly complex and capture noise in the training data as if it\n",
    "were true signal. They may overfit the training data.\n",
    "Impact: High variance models may have high accuracy on the training dataset but low accuracy on the test\n",
    "dataset.\n",
    "Examples: Decision trees, k-nearest neighbors with a low value of k.\n",
    "Comparison:\n",
    "\n",
    "Bias vs. Variance: Bias and variance are inversely related in the bias-variance tradeoff. Increasing model\n",
    "complexity reduces bias but increases variance, and vice versa.\n",
    "Performance: High bias models have low performance on both training and test datasets due to underfitting, \n",
    "while high variance models have high performance on the training dataset but low performance on the test\n",
    "dataset due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f20f4-7bef-48b5-bac1-d54bd7837a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 Regularization (Lasso):\n",
    "\n",
    "How it works: Adds the sum of the absolute values of the coefficients to the loss function.\n",
    "Effect: Encourages sparsity in the model, as it can force some coefficients to be exactly zero.\n",
    "Use case: Useful when there are many irrelevant features in the dataset.\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "How it works: Adds the sum of the squares of the coefficients to the loss function.\n",
    "Effect: Encourages smaller weights for all features, but rarely drives them to exactly zero.\n",
    "Use case: Helps prevent multicollinearity and reduces the impact of irrelevant features.\n",
    "Elastic Net Regularization:\n",
    "\n",
    "How it works: Combines both L1 and L2 regularization by adding both penalties to the loss function.\n",
    "Effect: Balances between sparsity (L1) and smoothness (L2) in the model.\n",
    "Use case: Useful when there are many features and some degree of feature selection is desired.\n",
    "Dropout (for neural networks):\n",
    "\n",
    "How it works: Randomly sets a fraction of the input units to zero during training.\n",
    "Effect: Prevents complex co-adaptations in the network, acting as a form of ensemble learning.\n",
    "Use case: Helps prevent overfitting in deep neural networks.\n",
    "Early Stopping:\n",
    "\n",
    "How it works: Monitors the model performance on a validation set during training and stops training when\n",
    "performance starts to degrade.\n",
    "Effect: Prevents the model from learning noise in the training data.\n",
    "Use case: Useful when training for a fixed number of epochs would lead to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
