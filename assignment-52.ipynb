{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64a384e-d82c-4efc-a4d7-ac67534b77ba",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique for regression tasks that builds an ensemble of decision trees sequentially. It is a popular and powerful method for building predictive models, known for its effectiveness in handling complex relationships in data and producing high-quality predictions. Here's how Gradient Boosting Regression works:\n",
    "\n",
    "Initialize the Model:\n",
    "\n",
    "Start with a simple model that predicts the average of the target variable for all instances.\n",
    "Fit a Weak Learner (Decision Tree):\n",
    "\n",
    "Train a decision tree to predict the residuals (the difference between the predicted values and the actual target values) of the current model.\n",
    "The decision tree is trained on the residuals, focusing on the instances where the current model performs poorly.\n",
    "Update the Model:\n",
    "\n",
    "Add the predictions of the decision tree to the current model, adjusting the predictions based on a learning rate (shrinkage) to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15327ec-e64f-4bae-8232-00692a2b278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 93.27030169608399\n",
      "R-squared: -1.7408078734217232\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.normal(size=100)\n",
    "\n",
    "# Define gradient boosting regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions to the mean of y\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "            # Fit a decision tree to residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            # Update predictions using learning rate\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            # Add the tree to the ensemble\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508bfde0-a9eb-4427-89cc-f631987760df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions to the mean of y\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "            # Fit a decision tree to residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            # Update predictions using learning rate\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            # Add the tree to the ensemble\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.models:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b542df-787d-4e24-855b-bd754daf6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best = best_model.predict(X)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best = mean_squared_error(y, y_pred_best)\n",
    "r2_best = r2_score(y, y_pred_best)\n",
    "\n",
    "print(\"Best Model Mean Squared Error:\", mse_best)\n",
    "print(\"Best Model R-squared:\", r2_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d6367-c0d2-43c7-8707-d8cbeb3e44d5",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a simple, often low-depth decision tree that performs slightly better than random guessing on a given dataset. Weak learners are typically used as base estimators in the boosting process. They are called \"weak\" because they alone are not very powerful and may have high bias or high variance.\n",
    "\n",
    "In the context of Gradient Boosting, the idea is to sequentially add these weak learners to the ensemble, each one focusing on the mistakes of the previous ones. By combining the predictions of many weak learners, Gradient Boosting can create a strong learner that achieves high performance on the task at hand.\n",
    "\n",
    "Weak learners are usually shallow decision trees with a limited number of nodes or depth, which helps prevent them from overfitting to the training data. Despite being individually weak, when combined appropriately, these learners can contribute significantly to the overall predictive power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1b1ab-c4d3-4db7-aa1f-122c32902994",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to sequentially build a series of weak learners (typically decision trees) and combine their predictions to create a strong learner. The key idea is to correct the errors made by the previous learners in the series, gradually improving the overall model's performance.\n",
    "\n",
    " Gradient Boosting builds a strong learner by iteratively improving upon the predictions of a series of weak learners, each one focusing on correcting the errors of its predecessors. This approach allows Gradient Boosting to create highly accurate models, especially when combined with techniques to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae84c3-5ee3-44f5-8659-11ca651124d2",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "Initialize the Ensemble:\n",
    "\n",
    "Start with a simple model that predicts the average value of the target variable for all instances. This serves as the initial prediction for the ensemble.\n",
    "Compute the Residuals:\n",
    "\n",
    "Compute the residuals (the difference between the predicted values and the actual target values) of the initial prediction. These residuals represent the errors that need to be corrected by subsequent weak learners.\n",
    "Build Sequential Weak Learners:\n",
    "\n",
    "For each iteration:\n",
    "Train a weak learner (e.g., decision tree) on the residuals of the current ensemble.\n",
    "The weak learner is trained to predict the residuals, focusing on instances where the current ensemble performs poorly.\n",
    "Add the weak learner to the ensemble, with a weight that determines its contribution to the final prediction.\n",
    "Update the Ensemble Predictions:\n",
    "\n",
    "Update the predictions of the ensemble by adding the prediction of the new weak learner, scaled by a learning rate.\n",
    "The learning rate controls the contribution of each weak learner to the final prediction. A lower learning rate makes the ensemble more robust to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f781e6-86ad-4c1a-9dfc-83daa3905afe",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "Define a differentiable loss function that measures the error between the predicted values and the actual target values. Common loss functions for regression problems include mean squared error (MSE) and absolute error.\n",
    "Initialize the Model:\n",
    "\n",
    "Start with a simple model that predicts the average of the target variable for all instances. This serves as the initial prediction for the ensemble.\n",
    "Compute Residuals:\n",
    "\n",
    "Compute the residuals (the difference between the predicted values and the actual target values) of the initial prediction. These residuals represent the errors that need to be corrected by subsequent weak learners.\n",
    "Sequential Learning:\n",
    "\n",
    "For each iteration:\n",
    "Train a weak learner (e.g., decision tree) on the residuals of the current ensemble.\n",
    "The weak learner is trained to predict the residuals, focusing on instances where the current ensemble performs poorly.\n",
    "Add the weak learner to the ensemble, with a weight that determines its contribution to the final prediction.\n",
    "Gradient Descent:\n",
    "\n",
    "Use gradient descent optimization to minimize the loss function. Calculate the negative gradient of the loss function with respect to the predictions of the current ensemble.\n",
    "Update the predictions of the ensemble by adding the negative gradient scaled by a learning rate. This step moves the predictions in the direction that reduces the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0834efa-5a5a-4a1a-bf17-69f669a819ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
