{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2a71b-67e4-4b58-96fd-615c06549d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Bayes theorem is a fundamental theorem in probability theory named after Thomas Bayes. \n",
    "It describes the probability of an event, based on prior knowledge of conditions that might be related \n",
    "to the event\n",
    "Bayes theorem is used in various fields, including statistics, machine learning, and artificial\n",
    "intelligence, for tasks such as Bayesian inference, Bayesian networks, and probabilistic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ac07f-66b1-4662-a944-91d39419fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "P(A/B)=(P(B/A)P(A))/P(B)\n",
    "\n",
    "P(A/B) is probability of A such that B has happened\n",
    "P(B/A)  is probability of B such that A has happened\n",
    "P(A) and P(B) are probabilities of A and B occured independently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182648a9-c8e0-4455-a64f-e778aad71ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayesian inference: It is used to update the probability of a hypothesis as more evidence or information\n",
    "becomes available.\n",
    "\n",
    "Medical diagnosis: Bayes theorem is used to calculate the probability that a patient has a particular\n",
    "disease given the results of a diagnostic test and the known prevalence of the disease in the population.\n",
    "\n",
    "Spam filtering: In email spam filtering, Bayes theorem is used to classify emails as spam or non-spam \n",
    "based on the presence or absence of certain words or features in the email.\n",
    "\n",
    "Machine learning: In machine learning, Bayes theorem is used in Naive Bayes classifiers, which are widely\n",
    "used for text classification tasks such as spam detection and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4d3d3-2032-4d87-90ac-b6a9133f4fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Bayes theorem is closely related to conditional probability. Conditional probability is the probability\n",
    "of an event occurring given that another event has already occurred, and it is denoted as\n",
    "P(A/B) read as \"the probability of A given B.\" Bayes theorem provides a way to calculate conditional\n",
    "probabilities using prior probabilities.\n",
    "The relationship between Bayes theorem and conditional probability can be seen in the formula for Bayes' \n",
    "theorem:\n",
    "P(A/B)=(P(B/A)P(A))/P(B)\n",
    "In essence, Bayes theorem allows us to calculate the conditional probability of an event based on prior\n",
    "probabilities and the likelihood of the event. It provides a way to update our beliefs about the\n",
    "probability of an event as new evidence or information becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32081c38-cec7-4ab7-85b2-848626db1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the right type of Naive Bayes classifier depends on the nature of your data and the assumptions\n",
    "you can make about its distribution. Here are some guidelines:\n",
    "\n",
    "Gaussian Naive Bayes: This is suitable for continuous data that approximately follows a Gaussian (normal)\n",
    "distribution.\n",
    "\n",
    "Multinomial Naive Bayes: This is commonly used for text classification problems, where the features are\n",
    "typically word counts or frequencies.\n",
    "\n",
    "Bernoulli Naive Bayes: This is useful when your features are binary (i.e., they take only two values, \n",
    "                                                                     such as 0/1 or True/False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce91c62-1d5f-48c7-9fc4-05f518043b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "To predict the class for the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we can \n",
    "calculate the likelihood of each class given these features and then use the prior probabilities to \n",
    "make the final prediction.\n",
    "\n",
    "The likelihood of each class given the features can be calculated as follows:\n",
    "\n",
    "For class A:\n",
    "P(X1=3 | A) = 4/13\n",
    "P(X2=4 | A) = 3/13\n",
    "P(A) = 1/2 (assuming equal prior probabilities for each class)\n",
    "\n",
    "Likelihood of A = P(X1=3 | A) * P(X2=4 | A) * P(A) = (4/13) * (3/13) * (1/2) â‰ˆ 0.046\n",
    "\n",
    "For class B:\n",
    "P(X1=3 | B) = 1/5\n",
    "P(X2=4 | B) = 3/5\n",
    "P(B) = 1/2 (assuming equal prior probabilities for each class)\n",
    "\n",
    "Likelihood of B = P(X1=3 | B) * P(X2=4 | B) * P(B) = (1/5) * (3/5) * (1/2) = 0.06\n",
    "\n",
    "Since the likelihood of class B is higher than that of class A, Naive Bayes would predict the new \n",
    "instance to belong to class B."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
