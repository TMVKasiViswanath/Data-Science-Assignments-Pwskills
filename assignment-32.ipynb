{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5188a12-123f-42a8-820a-fb5eeefb5ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression involves predicting a continuous target variable based on a single predictor \n",
    "variable. It assumes that there is a linear relationship between the predictor and the target.\n",
    "\n",
    "Example: Predicting a student score based on the number of hours they studied \n",
    "\n",
    "\n",
    "Multiple linear regression involves predicting a continuous target variable based on two or\n",
    "more predictor variables. It assumes a linear relationship between the predictors and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4621fe-320d-4d5e-b42b-3e006f2d8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linearity: The relationship between the predictors and the target variable is linear. This means that\n",
    "the change in the target variable is proportional to the change in the predictor variables.\n",
    "\n",
    "Independence: The errors (residuals) of the model are independent of each other. There should be no \n",
    "correlation between consecutive errors.\n",
    "\n",
    "Homoscedasticity: The variance of the errors is constant across all levels of the predictors. \n",
    "This implies that the spread of the residuals should be consistent along the range of predicted values.\n",
    "\n",
    "Normality of residuals: The residuals should be normally distributed. This means that the errors should\n",
    "follow a normal distribution with a mean of 0.\n",
    "\n",
    "No multicollinearity: There should be no multicollinearity among the predictor variables. This means\n",
    "that the predictors should be linearly independent of each other.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can perform several diagnostic tests:\n",
    "\n",
    "Residual plots: Plot the residuals against the predicted values. The plot should not show any clear\n",
    "patterns, indicating that the assumptions of linearity and homoscedasticity hold.\n",
    "\n",
    "Normal Q-Q plot: Plot the quantiles of the residuals against the quantiles of a normal distribution.\n",
    "The points should approximately follow a straight line, indicating that the residuals are normally \n",
    "distributed.\n",
    "\n",
    "Durbin-Watson test: This test checks for the presence of autocorrelation in the residuals. The test\n",
    "statistic ranges from 0 to 4, with values around 2 indicating no autocorrelation.\n",
    "\n",
    "Variance inflation factor (VIF): Calculate the VIF for each predictor variable to check for\n",
    "multicollinearity. VIF values greater than 10 indicate a high degree of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169d934-8c7b-46c3-8347-98c87e8a3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a linear regression model, the intercept represents the value of the target variable when all\n",
    "predictors are zero, while the slope represents the change in the target variable for a one-unit \n",
    "change in the predictor variable. For example, in a model predicting weight based on height, the \n",
    "intercept would be the estimated weight of a person with zero height (not meaningful), and the\n",
    "slope would represent the expected change in weight for a one-unit change in height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dd2f4-0853-48b6-a73e-c4948e9d0c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "In machine learning, gradient descent is used to minimize the cost function of a model by adjusting its \n",
    "parameters. The cost function measures how well the model predictions match the actual target values.\n",
    "By iteratively updating the model parameters in the direction that reduces the cost function, gradient\n",
    "descent helps the model learn the best values for its parameters.\n",
    "\n",
    "The basic steps of gradient descent are as follows:\n",
    "\n",
    "Initialize parameters: Start with initial values for the model parameters.\n",
    "\n",
    "Compute gradient: Calculate the gradient of the cost function with respect to each parameter.\n",
    "The gradient points in the direction of the steepest increase in the cost function.\n",
    "\n",
    "Update parameters: Update the parameters by taking a small step in the opposite direction of the gradient.\n",
    "The size of the step is controlled by a parameter called the learning rate.\n",
    "\n",
    "Repeat: Repeat steps 2 and 3 until the algorithm converges, i.e., the parameters reach values where the\n",
    "cost function is minimized or until a predefined number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e33ab9-56e9-402f-a6d0-157fecd26557",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of predictors: Simple linear regression has only one predictor variable,\n",
    "while multiple linear regression has two or more predictor variables.\n",
    "Model complexity: Multiple linear regression is more complex than simple linear regression because \n",
    "it considers the combined effect of multiple predictors on the response variable.\n",
    "Interpretation of coefficients: In multiple linear regression, the coefficients represent the change\n",
    "in the response variable for a one-unit change in each predictor, holding all other predictors constant.\n",
    "In simple linear regression, the coefficient represents the change in the response variable for a one-unit\n",
    "change in the predictor variable.\n",
    "Assumptions: The assumptions of multiple linear regression are similar to those of simple linear\n",
    "regression but extended to multiple predictors, including linearity, independence of errors, \n",
    "homoscedasticity, and normality of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8851a-188b-48e5-b53a-894f5c3aa828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more predictor \n",
    "variables are highly correlated, making it difficult to determine the individual effect of each predictor\n",
    "on the response variable. This can lead to unstable estimates of the regression coefficients and reduce\n",
    "the reliability of the model.\n",
    "\n",
    "Remove one of the correlated variables: If two or more variables are highly correlated, you can choose\n",
    "to remove one of them from the model.\n",
    "Combine the correlated variables: Instead of including both correlated variables, you can create a new\n",
    "variable that combines their information.\n",
    "Use regularization techniques: Regularization methods like Ridge or Lasso regression can help mitigate\n",
    "the effects of multicollinearity by penalizing large coefficients.\n",
    "Principal Component Analysis (PCA): PCA can be used to reduce the dimensionality of the data and remove\n",
    "multicollinearity by creating new uncorrelated variables (principal components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56389e7-3a38-4ccd-b651-53465321e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable \n",
    "x and the dependent variable \n",
    "y is modeled as an\n",
    "n-th degree polynomial.\n",
    "\n",
    "Model complexity: Polynomial regression is more complex than linear regression because it can capture\n",
    "non-linear relationships between variables.\n",
    "Number of predictors: In polynomial regression, the number of predictors increases with the degree of \n",
    "the polynomial, leading to a larger model with more terms.\n",
    "Interpretation of coefficients: The coefficients in polynomial regression represent the change in the\n",
    "dependent variable for a one-unit change in the independent variable raised to the corresponding power.\n",
    "The interpretation of these coefficients is more complex than in linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829274fc-ac3b-4f06-99da-98cce6039fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Captures Non-linear Relationships: Polynomial regression can model non-linear relationships between \n",
    "the independent and dependent variables, which linear regression cannot.\n",
    "Flexibility: It is more flexible in fitting a wider range of data patterns due to the higher degree of \n",
    "the polynomial.\n",
    "No Need for Transformation: In some cases, polynomial regression can handle skewed or non-normal data\n",
    "without the need for data transformation.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Higher-degree polynomials can lead to overfitting, where the model learns the noise in the\n",
    "data rather than the underlying pattern, resulting in poor generalization to new data.\n",
    "Interpretability: The interpretation of the model becomes more complex with higher-degree polynomials,\n",
    "making it harder to understand the effect of each predictor variable on the outcome.\n",
    "Computational Complexity: As the degree of the polynomial increases, the computational complexity of \n",
    "the model also increases, requiring more computational resources and time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
